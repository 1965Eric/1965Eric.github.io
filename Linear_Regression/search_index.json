[
["learning-objectives.html", "Data Science Linear Regression 1 Learning Objectives 1.1 Course Overview", " Data Science Linear Regression 1 Learning Objectives How linear regression was originally developed by Galton What confounding is and how to detect it How to examine the relationships between variables by implementing linear regression in R 1.1 Course Overview There are three major sections in this course: introduction to linear regression, linear models, and confounding. 1.1.1 Introduction to Linear Regression In this section, you’ll learn the basics of linear regression through this course’s motivating example, the data-driven approach used to construct baseball teams. You’ll also learn about correlation, the correlation coefficient, stratification, and the variance explained. 1.1.2 Linear Models In this section, you’ll learn about linear models. You’ll learn about least squares estimates, multivariate regression, and several useful features of R, such as tibbles, lm, do, and broom. You’ll learn how to apply regression to baseball to build a better offensive metric. 1.1.3 Confounding In the final section of the course, you’ll learn about confounding and several reasons that correlation is not the same as causation, such as spurious correlation, outliers, reversing cause and effect, and confounders. You’ll also learn about Simpson’s Paradox. "],
["section-1-introduction-to-regression-overview.html", "2 Section 1 - Introduction to Regression Overview 2.1 Motivating Example: Moneyball 2.2 Baseball basics 2.3 Bases on Balls or Stolen Bases? 2.4 Assessment - Baseball as a Motivating Example 2.5 Correlation 2.6 Correlation Coefficient 2.7 Sample Correlation is a Random Variable 2.8 Assessment - Correlation 2.9 Anscombe’s Quartet/Stratification 2.10 Bivariate Normal Distribution 2.11 Variance Explained 2.12 There are Two Regression Lines 2.13 Assessment - Stratification and Variance Explained, Part 1 2.14 Assessment - Stratification and Variance Explained, Part 2", " 2 Section 1 - Introduction to Regression Overview In the Introduction to Regression section, you will learn the basics of linear regression. After completing this section, you will be able to: Understand how Galton developed linear regression. Calculate and interpret the sample correlation. Stratify a dataset when appropriate. Understand what a bivariate normal distribution is. Explain what the term variance explained means. Interpret the two regression lines. This section has three parts: Baseball as a Motivating Example, Correlation, and Stratification and Variance Explained. 2.1 Motivating Example: Moneyball The corresponding section of the textbook is the case study on Moneyball Key points Bill James was the originator of the sabermetrics, the approach of using data to predict what outcomes best predicted if a team would win. 2.2 Baseball basics The corresponding section of the textbook is the section on baseball basics Key points The goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters who have an opportunity to hit a ball with a bat in a predetermined order. Each time a batter has an opportunity to bat, we call it a plate appearance (PA). The PA ends with a binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases). We are simplifying a bit, but there are five ways a batter can succeed (not make an out): Bases on balls (BB): the pitcher fails to throw the ball through a predefined area considered to be hittable (the strike zone), so the batter is permitted to go to first base. Single: the batter hits the ball and gets to first base. Double (2B): the batter hits the ball and gets to second base. Triple (3B): the batter hits the ball and gets to third base. Home Run (HR): the batter hits the ball and goes all the way home and scores a run. Historically, the batting average has been considered the most important offensive statistic. To define this average, we define a hit (H) and an at bat (AB). Singles, doubles, triples and home runs are hits. The fifth way to be successful, a walk (BB), is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate. 2.3 Bases on Balls or Stolen Bases? The corresponding section of the textbook is the base on balls or stolen bases textbook section Key points The visualization of choice when exploring the relationship between two variables like home runs and runs is a scatterplot. Code: Scatterplot of the relationship between HRs and runs if(!require(Lahman)) install.packages(&quot;Lahman&quot;) ## Loading required package: Lahman if(!require(tidyverse)) install.packages(&quot;tidyverse&quot;) if(!require(dslabs)) install.packages(&quot;dslabs&quot;) library(Lahman) library(tidyverse) library(dslabs) ds_theme_set() Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(HR_per_game = HR / G, R_per_game = R / G) %&gt;% ggplot(aes(HR_per_game, R_per_game)) + geom_point(alpha = 0.5) Code: Scatterplot of the relationship between stolen bases and runs Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(SB_per_game = SB / G, R_per_game = R / G) %&gt;% ggplot(aes(SB_per_game, R_per_game)) + geom_point(alpha = 0.5) Code: Scatterplot of the relationship between bases on balls and runs Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(BB_per_game = BB / G, R_per_game = R / G) %&gt;% ggplot(aes(BB_per_game, R_per_game)) + geom_point(alpha = 0.5) 2.4 Assessment - Baseball as a Motivating Example What is the application of statistics and data science to baseball called? A. Moneyball B. Sabermetrics C. The “Oakland A’s Approach” D. There is no specific name for this; it’s just data science. Which of the following outcomes is not included in the batting average? A. A home run B. A base on balls C. An out D. A single Why do we consider team statistics as well as individual player statistics? A. The success of any individual player also depends on the strength of their team. B. Team statistics can be easier to calculate. C. The ultimate goal of sabermetrics is to rank teams, not players. You want to know whether teams with more at-bats per game have more runs per game. What R code below correctly makes a scatter plot for this relationship? Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(AB_per_game = AB/G, R_per_game = R/G) %&gt;% ggplot(aes(AB_per_game, R_per_game)) + geom_point(alpha = 0.5) A. Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% ggplot(aes(AB, R)) + geom_point(alpha = 0.5) B. Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(AB_per_game = AB/G, R_per_game = R/G) %&gt;% ggplot(aes(AB_per_game, R_per_game)) + geom_point(alpha = 0.5) C. Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(AB_per_game = AB/G, R_per_game = R/G) %&gt;% ggplot(aes(AB_per_game, R_per_game)) + geom_line() D. Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(AB_per_game = AB/G, R_per_game = R/G) %&gt;% ggplot(aes(R_per_game, AB_per_game)) + geom_point() What does the variable “SOA” stand for in the Teams table? Hint: make sure to use the help file (?Teams). A. sacrifice out B. slides or attempts C. strikeouts by pitchers D. accumulated singles Load the Lahman library. Filter the Teams data frame to include years from 1961 to 2001. Make a scatterplot of runs per game versus at bats (AB) per game. Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(AB_per_game = AB / G, R_per_game = R / G) %&gt;% ggplot(aes(AB_per_game, R_per_game)) + geom_point(alpha = 0.5) Which of the following is true? A. There is no clear relationship between runs and at bats per game. B. As the number of at bats per game increases, the number of runs per game tends to increase. C. As the number of at bats per game increases, the number of runs per game tends to decrease. Use the filtered Teams data frame from Question 6. Make a scatterplot of win rate (number of wins per game) versus number of fielding errors (E) per game. Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(win_rate = W / G, E_per_game = E / G) %&gt;% ggplot(aes(win_rate, E_per_game)) + geom_point(alpha = 0.5) Which of the following is true? A. There is no relationship between win rate and errors per game. B. As the number of errors per game increases, the win rate tends to increase. C. As the number of errors per game increases, the win rate tends to decrease. Use the filtered Teams data frame from Question 6. Make a scatterplot of triples (X3B) per game versus doubles (X2B) per game. Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(doubles_per_game = X2B / G, triples_per_game = X3B / G) %&gt;% ggplot(aes(doubles_per_game, triples_per_game)) + geom_point(alpha = 0.5) Which of the following is true? A. There is no clear relationship between doubles per game and triples per game. B. As the number of doubles per game increases, the number of triples per game tends to increase. C. As the number of doubles per game increases, the number of triples per game tends to decrease. 2.5 Correlation The corresponding textbook section is Case Study: is height hereditary? Key points Galton tried to predict sons’ heights based on fathers’ heights. The mean and standard errors are insufficient for describing an important characteristic of the data: the trend that the taller the father, the taller the son. The correlation coefficient is an informative summary of how two variables move together that can be used to predict one variable using the other. Code # create the dataset if(!require(HistData)) install.packages(&quot;HistData&quot;) ## Loading required package: HistData library(HistData) data(&quot;GaltonFamilies&quot;) set.seed(1983) galton_heights &lt;- GaltonFamilies %&gt;% filter(gender == &quot;male&quot;) %&gt;% group_by(family) %&gt;% sample_n(1) %&gt;% ungroup() %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) # means and standard deviations galton_heights %&gt;% summarize(mean(father), sd(father), mean(son), sd(son)) ## # A tibble: 1 x 4 ## `mean(father)` `sd(father)` `mean(son)` `sd(son)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 69.1 2.55 69.2 2.71 # scatterplot of father and son heights galton_heights %&gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) 2.6 Correlation Coefficient The corresponding textbook section is the correlation coefficient Key points The correlation coefficient is defined for a list of pairs \\((x_1, y_1), ..., (x_n, y_n)\\) as the product of the standardized values: \\((\\frac{x_i - \\mu_x}{\\sigma_x})(\\frac{y_i -\\mu_y}{\\sigma_y})\\). The correlation coefficient essentially conveys how two variables move together. The correlation coefficient is always between -1 and 1. Code rho &lt;- mean(scale(x)*scale(y)) galton_heights &lt;- GaltonFamilies %&gt;% filter(childNum == 1 &amp; gender == &quot;male&quot;) %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) galton_heights %&gt;% summarize(r = cor(father, son)) %&gt;% pull(r) ## [1] 0.5007248 2.7 Sample Correlation is a Random Variable The corresponding textbook section is Sample correlation is a random variable Key points The correlation that we compute and use as a summary is a random variable. When interpreting correlations, it is important to remember that correlations derived from samples are estimates containing uncertainty. Because the sample correlation is an average of independent draws, the central limit theorem applies. Code # compute sample correlation R &lt;- sample_n(galton_heights, 25, replace = TRUE) %&gt;% summarize(r = cor(father, son)) R ## r ## 1 0.4787613 # Monte Carlo simulation to show distribution of sample correlation B &lt;- 1000 N &lt;- 25 R &lt;- replicate(B, { sample_n(galton_heights, N, replace = TRUE) %&gt;% summarize(r = cor(father, son)) %&gt;% pull(r) }) qplot(R, geom = &quot;histogram&quot;, binwidth = 0.05, color = I(&quot;black&quot;)) # expected value and standard error mean(R) ## [1] 0.4970997 sd(R) ## [1] 0.1512451 # QQ-plot to evaluate whether N is large enough data.frame(R) %&gt;% ggplot(aes(sample = R)) + stat_qq() + geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2))) 2.8 Assessment - Correlation While studying heredity, Francis Galton developed what important statistical concept? A. Standard deviation B. Normal distribution C. Correlation D. Probability The correlation coefficient is a summary of what? A. The trend between two variables B. The dispersion of a variable C. The central tendency of a variable D. The distribution of a variable Below is a scatter plot showing the relationship between two variables, x and y. Scatter plot relationship x and y From this figure, the correlation between x and y appears to be about: A. -0.9 B. -0.2 C. 0.9 D. 2 Instead of running a Monte Carlo simulation with a sample size of 25 from our 179 father-son pairs, we now run our simulation with a sample size of 50. Would you expect the mean of our sample correlation to increase, decrease, or stay approximately the same? A. Increase B. Decrease C. Stay approximately the same Instead of running a Monte Carlo simulation with a sample size of 25 from our 179 father-son pairs, we now run our simulation with a sample size of 50. Would you expect the standard deviation of our sample correlation to increase, decrease, or stay approximately the same? A. Increase B. Decrease C. Stay approximately the same If X and Y are completely independent, what do you expect the value of the correlation coefficient to be? A. -1 B. -0.5 C. 0 D. 0.5 E. 1 F. Not enough information to answer the question Load the Lahman library. Filter the Teams data frame to include years from 1961 to 2001. What is the correlation coefficient between number of runs per game and number of at bats per game? Teams_small &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) cor(Teams_small$R/Teams_small$G, Teams_small$AB/Teams_small$G) ## [1] 0.6580976 Use the filtered Teams data frame from Question 7. What is the correlation coefficient between win rate (number of wins per game) and number of errors per game? cor(Teams_small$W/Teams_small$G, Teams_small$E/Teams_small$G) ## [1] -0.3396947 Use the filtered Teams data frame from Question 7. What is the correlation coefficient between doubles (X2B) per game and triples (X3B) per game? cor(Teams_small$X2B/Teams_small$G, Teams_small$X3B/Teams_small$G) ## [1] -0.01157404 2.9 Anscombe’s Quartet/Stratification There are three links to relevant sections of the textbook: Correlation is not always a useful summary Conditional expectation The regression line Key points Correlation is not always a good summary of the relationship between two variables. The general idea of conditional expectation is that we stratify a population into groups and compute summaries in each group. A practical way to improve the estimates of the conditional expectations is to define strata of with similar values of x. If there is perfect correlation, the regression line predicts an increase that is the same number of SDs for both variables. If there is 0 correlation, then we don’t use x at all for the prediction and simply predict the average \\(\\mu_y\\). For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase. Code # number of fathers with height 72 or 72.5 inches sum(galton_heights$father == 72) ## [1] 8 sum(galton_heights$father == 72.5) ## [1] 1 # predicted height of a son with a 72 inch tall father conditional_avg &lt;- galton_heights %&gt;% filter(round(father) == 72) %&gt;% summarize(avg = mean(son)) %&gt;% pull(avg) conditional_avg ## [1] 71.83571 # stratify fathers&#39; heights to make a boxplot of son heights galton_heights %&gt;% mutate(father_strata = factor(round(father))) %&gt;% ggplot(aes(father_strata, son)) + geom_boxplot() + geom_point() # center of each boxplot galton_heights %&gt;% mutate(father = round(father)) %&gt;% group_by(father) %&gt;% summarize(son_conditional_avg = mean(son)) %&gt;% ggplot(aes(father, son_conditional_avg)) + geom_point() ## `summarise()` ungrouping output (override with `.groups` argument) # calculate values to plot regression line on original data mu_x &lt;- mean(galton_heights$father) mu_y &lt;- mean(galton_heights$son) s_x &lt;- sd(galton_heights$father) s_y &lt;- sd(galton_heights$son) r &lt;- cor(galton_heights$father, galton_heights$son) m &lt;- r * s_y/s_x b &lt;- mu_y - m*mu_x # add regression line to plot galton_heights %&gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) + geom_abline(intercept = b, slope = m) 2.10 Bivariate Normal Distribution There is a link to the relevant section of the textbook: Bivariate normal distribution (advanced) Key points When a pair of random variables are approximated by the bivariate normal distribution, scatterplots look like ovals. They can be thin (high correlation) or circle-shaped (no correlation). When two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to make predictions. Code galton_heights %&gt;% mutate(z_father = round((father - mean(father)) / sd(father))) %&gt;% filter(z_father %in% -2:2) %&gt;% ggplot() + stat_qq(aes(sample = son)) + facet_wrap( ~ z_father) 2.11 Variance Explained There is a link to the relevant section of the textbook: Variance explained Key points Conditioning on a random variable X can help to reduce variance of response variable Y. The standard deviation of the conditional distribution is \\(\\mbox{SD}(Y \\mid X=x) = \\sigma_y\\sqrt{1-\\rho^2}\\), which is smaller than the standard deviation without conditioning \\(\\sigma_y\\). Because variance is the standard deviation squared, the variance of the conditional distribution is \\(\\sigma_y^2(1-\\rho^2)\\). In the statement “X explains such and such percent of the variability,” the percent value refers to the variance. The variance decreases by \\(\\rho^2\\) percent. The “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution. 2.12 There are Two Regression Lines There is a link to the relevant section of the textbook: Warning: there are two regression lines Key point There are two different regression lines depending on whether we are taking the expectation of Y given X or taking the expectation of X given Y. Code # compute a regression line to predict the son&#39;s height from the father&#39;s height mu_x &lt;- mean(galton_heights$father) mu_y &lt;- mean(galton_heights$son) s_x &lt;- sd(galton_heights$father) s_y &lt;- sd(galton_heights$son) r &lt;- cor(galton_heights$father, galton_heights$son) m_1 &lt;- r * s_y / s_x b_1 &lt;- mu_y - m_1*mu_x m_1 # slope 1 ## [1] 0.5027904 b_1 # intercept 1 ## [1] 35.71249 # compute a regression line to predict the father&#39;s height from the son&#39;s height m_2 &lt;- r * s_x / s_y b_2 &lt;- mu_x - m_2*mu_y m_2 # slope 2 ## [1] 0.4986676 b_2 # intercept 2 ## [1] 33.96539 2.13 Assessment - Stratification and Variance Explained, Part 1 Look at the figure below. The slope of the regression line in this figure is equal to what, in words? Scatter plot and regression line of son and father heights A. Slope = (correlation coefficient of son and father heights) * (standard deviation of sons’ heights / standard deviation of fathers’ heights) B. Slope = (correlation coefficient of son and father heights) * (standard deviation of fathers’ heights / standard deviation of sons’ heights) C. Slope = (correlation coefficient of son and father heights) / (standard deviation of sons’ heights * standard deviation of fathers’ heights) D. Slope = (mean height of fathers) - (correlation coefficient of son and father heights * mean height of sons). Why does the regression line simplify to a line with intercept zero and slope \\(\\rho\\) when we standardize our x and y variables? Try the simplification on your own first! A. When we standardize variables, both x and y will have a mean of one and a standard deviation of zero. When you substitute this into the formula for the regression line, the terms cancel out until we have the following equation: \\(y_i = \\rho x_i\\). B. When we standardize variables, both x and y will have a mean of zero and a standard deviation of one. When you substitute this into the formula for the regression line, the terms cancel out until we have the following equation: \\(y_i = \\rho x_i\\). C. When we standardize variables, both x and y will have a mean of zero and a standard deviation of one. When you substitute this into the formula for the regression line, the terms cancel out until we have the following equation: \\(y_i = \\rho + x_i\\). What is a limitation of calculating conditional means? A. Each stratum we condition on (e.g., a specific father’s height) may not have many data points. B. Because there are limited data points for each stratum, our average values have large standard errors. C. Conditional means are less stable than a regression line. D. Conditional means are a useful theoretical tool but cannot be calculated. A regression line is the best prediction of Y given we know the value of X when: A. X and Y follow a bivariate normal distribution. B. Both X and Y are normally distributed. C. Both X and Y have been standardized. D. There are at least 25 X-Y pairs. Which one of the following scatterplots depicts an x and y distribution that is NOT well-approximated by the bivariate normal distribution? A. B. C. D. We previously calculated that the correlation coefficient between fathers’ and sons’ heights is 0.5. Given this, what percent of the variation in sons’ heights is explained by fathers’ heights? A. 0% B. 25% C. 50% D. 75% When two variables follow a bivariate normal distribution, the variation explained can be calculated as \\(\\rho^2 \\times 100\\). Suppose the correlation between father and son’s height is 0.5, the standard deviation of fathers’ heights is 2 inches, and the standard deviation of sons’ heights is 3 inches. Given a one inch increase in a father’s height, what is the predicted change in the son’s height? A. 0.333 B. 0.5 C. 0.667 D. 0.75 E. 1 F. 1.5 The slope of the regression line is calculated by multiplying the correlation coefficient by the ratio of the standard deviation of son heights and standard deviation of father heights: \\(\\sigma_{son}/\\sigma_{father}\\). 2.14 Assessment - Stratification and Variance Explained, Part 2 In the second part of this assessment, you’ll analyze a set of mother and daughter heights, also from GaltonFamilies. Define female_heights, a set of mother and daughter heights sampled from GaltonFamilies, as follows: set.seed(1989, sample.kind=&quot;Rounding&quot;) #if you are using R 3.6 or later ## Warning in set.seed(1989, sample.kind = &quot;Rounding&quot;): non-uniform &#39;Rounding&#39; sampler used female_heights &lt;- GaltonFamilies%&gt;% filter(gender == &quot;female&quot;) %&gt;% group_by(family) %&gt;% sample_n(1) %&gt;% ungroup() %&gt;% select(mother, childHeight) %&gt;% rename(daughter = childHeight) Calculate the mean and standard deviation of mothers’ heights, the mean and standard deviation of daughters’ heights, and the correlaton coefficient between mother and daughter heights. Mean of mothers’ heights mean(female_heights$mother) ## [1] 64.125 Standard deviation of mothers’ heights sd(female_heights$mother) ## [1] 2.289292 Mean of daughters’ heights mean(female_heights$daughter) ## [1] 64.28011 Standard deviation of daughters’ heights sd(female_heights$daughter) ## [1] 2.39416 Correlation coefficient cor(female_heights$mother, female_heights$daughter) ## [1] 0.3245199 Calculate the slope and intercept of the regression line predicting daughters’ heights given mothers’ heights. Given an increase in mother’s height by 1 inch, how many inches is the daughter’s height expected to change? Slope of regression line predicting daughters’ height from mothers’ heights r &lt;- cor(female_heights$mother, female_heights$daughter) s_y &lt;- sd(female_heights$daughter) s_x &lt;- sd(female_heights$mother) r * s_y/s_x ## [1] 0.3393856 Intercept of regression line predicting daughters’ height from mothers’ heights mu_y &lt;- mean(female_heights$daughter) mu_x &lt;- mean(female_heights$mother) mu_y - (r * s_y/s_x)*mu_x ## [1] 42.51701 Change in daughter’s height in inches given a 1 inch increase in the mother’s height r * s_y/s_x ## [1] 0.3393856 What percent of the variability in daughter heights is explained by the mother’s height? r^2*100 ## [1] 10.53132 A mother has a height of 60 inches. What is the conditional expected value of her daughter’s height given the mother’s height? m = r * s_y/s_x b = mu_y - (r * s_y/s_x)*mu_x x = 60 m*x+b ## [1] 62.88015 "],
["section-2-linear-models-overview.html", "3 Section 2 - Linear Models Overview 3.1 Confounding: Are BBs More Predictive? 3.2 Stratification and Multivariate Regression 3.3 Linear Models 3.4 Assessment: Introduction to Linear Models 3.5 Least Squares Estimates (LSE) 3.6 The lm Function 3.7 LSE are Random Variables 3.8 Advanced Note on LSE 3.9 Predicted Variables are Random Variables 3.10 Assessment - Least Squares Estimates (LSE), part 1 3.11 Assessment - Least Squares Estimates, part 2 3.12 Advanced dplyr: Tibbles 3.13 Tibbles: Differences from Data Frames 3.14 do 3.15 broom 3.16 Assessment - Tibbles, do, and broom, part 1 3.17 Assessment - Tibbles, do, and broom, part 2 3.18 Building a Better Offensive Metric for Baseball 3.19 Building a Better Offensive Metric for Baseball: Linear Programming 3.20 On Base Plus Slugging (OPS) 3.21 Regression Fallacy 3.22 Measurement Error Models 3.23 Assessment - Regression and Baseball, part 1 3.24 Assessment - Regression and Baseball, part 2 3.25 Assessment - Linear Models", " 3 Section 2 - Linear Models Overview In the Linear Models section, you will learn how to do linear regression. After completing this section, you will be able to: Use multivariate regression to adjust for confounders. Write linear models to describe the relationship between two or more variables. Calculate the least squares estimates for a regression model using the lm function. Understand the differences between tibbles and data frames. Use the do() function to bridge R functions and the tidyverse. Use the tidy(), glance(), and augment() functions from the broom package. Apply linear regression to measurement error models. This section has four parts: Introduction to Linear Models, Least Squares Estimates, Tibbles, do, and broom, and Regression and Baseball. 3.1 Confounding: Are BBs More Predictive? The textbook for this section is available here Key points Association is not causation! Although it may appear that BB cause runs, it is actually the HR that cause most of these runs. We say that BB are confounded with HR. Regression can help us account for confounding. Code # find regression line for predicting runs from BBs bb_slope &lt;- Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(BB_per_game = BB/G, R_per_game = R/G) %&gt;% lm(R_per_game ~ BB_per_game, data = .) %&gt;% .$coef %&gt;% .[2] bb_slope ## BB_per_game ## 0.7353288 # compute regression line for predicting runs from singles singles_slope &lt;- Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %&gt;% lm(R_per_game ~ Singles_per_game, data = .) %&gt;% .$coef %&gt;% .[2] singles_slope ## Singles_per_game ## 0.4494253 # calculate correlation between HR, BB and singles Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;% mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %&gt;% summarize(cor(BB, HR), cor(Singles, HR), cor(BB,Singles)) ## cor(BB, HR) cor(Singles, HR) cor(BB, Singles) ## 1 0.4039313 -0.1737435 -0.05603822 3.2 Stratification and Multivariate Regression The textbook for this section is available here Key points A first approach to check confounding is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. The slopes of BB after stratifying on HR are reduced, but they are not 0, which indicates that BB are helpful for producing runs, just not as much as previously thought. Code # stratify HR per game to nearest 10, filter out strata with few points dat &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(HR_strata = round(HR/G, 1), BB_per_game = BB / G, R_per_game = R / G) %&gt;% filter(HR_strata &gt;= 0.4 &amp; HR_strata &lt;=1.2) # scatterplot for each HR stratum dat %&gt;% ggplot(aes(BB_per_game, R_per_game)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;) + facet_wrap( ~ HR_strata) ## `geom_smooth()` using formula &#39;y ~ x&#39; # calculate slope of regression line after stratifying by HR dat %&gt;% group_by(HR_strata) %&gt;% summarize(slope = cor(BB_per_game, R_per_game)*sd(R_per_game)/sd(BB_per_game)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 9 x 2 ## HR_strata slope ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.4 0.734 ## 2 0.5 0.566 ## 3 0.6 0.412 ## 4 0.7 0.285 ## 5 0.8 0.365 ## 6 0.9 0.261 ## 7 1 0.512 ## 8 1.1 0.454 ## 9 1.2 0.440 # stratify by BB dat &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(BB_strata = round(BB/G, 1), HR_per_game = HR / G, R_per_game = R / G) %&gt;% filter(BB_strata &gt;= 2.8 &amp; BB_strata &lt;=3.9) # scatterplot for each BB stratum dat %&gt;% ggplot(aes(HR_per_game, R_per_game)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;) + facet_wrap( ~ BB_strata) ## `geom_smooth()` using formula &#39;y ~ x&#39; # slope of regression line after stratifying by BB dat %&gt;% group_by(BB_strata) %&gt;% summarize(slope = cor(HR_per_game, R_per_game)*sd(R_per_game)/sd(HR_per_game)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 12 x 2 ## BB_strata slope ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2.8 1.52 ## 2 2.9 1.57 ## 3 3 1.52 ## 4 3.1 1.49 ## 5 3.2 1.58 ## 6 3.3 1.56 ## 7 3.4 1.48 ## 8 3.5 1.63 ## 9 3.6 1.83 ## 10 3.7 1.45 ## 11 3.8 1.70 ## 12 3.9 1.30 3.3 Linear Models The textbook for this section is available here Key points “Linear” here does not refer to lines, but rather to the fact that the conditional expectation is a linear combination of known quantities. In Galton’s model, we assume \\(Y\\) (son’s height) is a linear combination of a constant and \\(X\\) (father’s height) plus random noise. We further assume that \\(\\epsilon_i\\) are independent from each other, have expected value 0 and the standard deviation \\(\\sigma\\) which does not depend on i. Note that if we further assume that \\(\\epsilon\\) is normally distributed, then the model is exactly the same one we derived earlier by assuming bivariate normal data. We can subtract the mean from \\(X\\) to make \\(\\beta_0\\) more interpretable. 3.4 Assessment: Introduction to Linear Models When we stratified our regression lines for runs per game vs. bases on balls by the number of home runs, what happened? A. The slope of runs per game vs. bases on balls within each stratum was reduced because we removed confounding by home runs. B. The slope of runs per game vs. bases on balls within each stratum was reduced because there were fewer data points. C. The slope of runs per game vs. bases on balls within each stratum increased after we removed confounding by home runs. D. The slope of runs per game vs. bases on balls within each stratum stayed about the same as the original slope. We run a linear model for sons’ heights vs. fathers’ heights using the Galton height data, and get the following results: &gt; lm(son ~ father, data = galton_heights) Call: lm(formula = son ~ father, data = galton_heights) Coefficients: (Intercept) father 35.71 0.50 Interpret the numeric coefficient for “father.” A. For every inch we increase the son’s height, the predicted father’s height increases by 0.5 inches. B. For every inch we increase the father’s height, the predicted son’s height grows by 0.5 inches. C. For every inch we increase the father’s height, the predicted son’s height is 0.5 times greater. We want the intercept term for our model to be more interpretable, so we run the same model as before but now we subtract the mean of fathers’ heights from each individual father’s height to create a new variable centered at zero. galton_heights &lt;- galton_heights %&gt;% mutate(father_centered=father - mean(father)) We run a linear model using this centered fathers’ height variable. &gt; lm(son ~ father_centered, data = galton_heights) Call: lm(formula = son ~ father_centered, data = galton_heights) Coefficients: (Intercept) father_centered 70.45 0.50 Interpret the numeric coefficient for the intercept. A. The height of a son of a father of average height is 70.45 inches. B. The height of a son when a father’s height is zero is 70.45 inches. C. The height of an average father is 70.45 inches. Suppose we fit a multivariate regression model for expected runs based on BB and HR: \\(E[R|BB=x_1, HR=x_2] = \\beta_0+\\beta_1x_1+\\beta_2x_2\\) Suppose we fix \\(BB = x_1\\). Then we observe a linear relationship between runs and HR with intercept of: A. \\(\\beta_0\\) B. \\(\\beta_0 + \\beta_2x_2\\) C. \\(\\beta_0 + \\beta_1x_1\\) D. \\(\\beta_0 + \\beta_2x_1\\) Which of the following are assumptions for the errors \\(\\epsilon_i\\) in a linear regression model? Check ALL correct answers. A. The \\(\\epsilon_i\\) are independent of each other B. The \\(\\epsilon_i\\) have expected value 0 C. The variance of \\(\\epsilon_i\\) is a constant 3.5 Least Squares Estimates (LSE) The textbook for this section is available here Key points For regression, we aim to find the coefficient values that minimize the distance of the fitted model to the data. Residual sum of squares (RSS) measures the distance between the true value and the predicted value given by the regression line. The values that minimize the RSS are called the least squares estimates (LSE). We can use partial derivatives to get the values for \\(\\beta_0\\) and \\(\\beta_1\\) in Galton’s data. Code # compute RSS for any pair of beta0 and beta1 in Galton&#39;s data set.seed(1983) galton_heights &lt;- GaltonFamilies %&gt;% filter(gender == &quot;male&quot;) %&gt;% group_by(family) %&gt;% sample_n(1) %&gt;% ungroup() %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) rss &lt;- function(beta0, beta1, data){ resid &lt;- galton_heights$son - (beta0+beta1*galton_heights$father) return(sum(resid^2)) } # plot RSS as a function of beta1 when beta0=25 beta1 = seq(0, 1, len=nrow(galton_heights)) results &lt;- data.frame(beta1 = beta1, rss = sapply(beta1, rss, beta0 = 25)) results %&gt;% ggplot(aes(beta1, rss)) + geom_line() + geom_line(aes(beta1, rss)) 3.6 The lm Function The textbook for this section is available here Key points When calling the lm() function, the variable that we want to predict is put to the left of the ~ symbol, and the variables that we use to predict is put to the right of the ~ symbol. The intercept is added automatically. LSEs are random variables. Code # fit regression line to predict son&#39;s height from father&#39;s height fit &lt;- lm(son ~ father, data = galton_heights) fit ## ## Call: ## lm(formula = son ~ father, data = galton_heights) ## ## Coefficients: ## (Intercept) father ## 38.7646 0.4411 # summary statistics summary(fit) ## ## Call: ## lm(formula = son ~ father, data = galton_heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.4228 -1.7022 0.0333 1.5670 9.3567 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.76457 5.41093 7.164 2.03e-11 *** ## father 0.44112 0.07825 5.637 6.72e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.659 on 177 degrees of freedom ## Multiple R-squared: 0.1522, Adjusted R-squared: 0.1474 ## F-statistic: 31.78 on 1 and 177 DF, p-value: 6.719e-08 3.7 LSE are Random Variables The textbook for this section is available here Key points Because they are derived from the samples, LSE are random variables. \\(\\beta_0\\) and \\(\\beta_1\\) appear to be normally distributed because the central limit theorem plays a role. The t-statistic depends on the assumption that \\(\\epsilon\\) follows a normal distribution. Code # Monte Carlo simulation B &lt;- 1000 N &lt;- 50 lse &lt;- replicate(B, { sample_n(galton_heights, N, replace = TRUE) %&gt;% lm(son ~ father, data = .) %&gt;% .$coef }) lse &lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) # Plot the distribution of beta_0 and beta_1 if(!require(gridExtra)) install.packages(&quot;gridExtra&quot;) ## Loading required package: gridExtra ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine library(gridExtra) p1 &lt;- lse %&gt;% ggplot(aes(beta_0)) + geom_histogram(binwidth = 5, color = &quot;black&quot;) p2 &lt;- lse %&gt;% ggplot(aes(beta_1)) + geom_histogram(binwidth = 0.1, color = &quot;black&quot;) grid.arrange(p1, p2, ncol = 2) # summary statistics sample_n(galton_heights, N, replace = TRUE) %&gt;% lm(son ~ father, data = .) %&gt;% summary %&gt;% .$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.4729422 8.6021831 4.007464 0.0002129225 ## father 0.4990193 0.1240572 4.022493 0.0002030210 lse %&gt;% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1)) ## se_0 se_1 ## 1 9.683973 0.1411404 3.8 Advanced Note on LSE Although interpretation is not straight-forward, it is also useful to know that the LSE can be strongly correlated, which can be seen using this code: lse %&gt;% summarize(cor(beta_0, beta_1)) ## cor(beta_0, beta_1) ## 1 -0.9993386 However, the correlation depends on how the predictors are defined or transformed. Here we standardize the father heights, which changes \\(x_i\\) to \\(x_i - \\bar{x}\\). B &lt;- 1000 N &lt;- 50 lse &lt;- replicate(B, { sample_n(galton_heights, N, replace = TRUE) %&gt;% mutate(father = father - mean(father)) %&gt;% lm(son ~ father, data = .) %&gt;% .$coef }) Observe what happens to the correlation in this case: cor(lse[1,], lse[2,]) ## [1] 0.1100929 3.9 Predicted Variables are Random Variables The textbook for this section is available here Key points The predicted value is often denoted as \\(\\hat{Y}\\), which is a random variable. Mathematical theory tells us what the standard error of the predicted value is. The predict() function in R can give us predictions directly. Code # plot predictions and confidence intervals galton_heights %&gt;% ggplot(aes(son, father)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; # predict Y directly fit &lt;- galton_heights %&gt;% lm(son ~ father, data = .) Y_hat &lt;- predict(fit, se.fit = TRUE) names(Y_hat) ## [1] &quot;fit&quot; &quot;se.fit&quot; &quot;df&quot; &quot;residual.scale&quot; # plot best fit line galton_heights %&gt;% mutate(Y_hat = predict(lm(son ~ father, data=.))) %&gt;% ggplot(aes(father, Y_hat))+ geom_line() 3.10 Assessment - Least Squares Estimates (LSE), part 1 The following code was used in the video to plot RSS with \\(\\beta_0=25\\). beta1 = seq(0, 1, len=nrow(galton_heights)) results &lt;- data.frame(beta1 = beta1, rss = sapply(beta1, rss, beta0 = 25)) results %&gt;% ggplot(aes(beta1, rss)) + geom_line() + geom_line(aes(beta1, rss), col=2) In a model for sons’ heights vs fathers’ heights, what is the least squares estimate (LSE) for \\(\\beta_1\\) if we assume \\(\\hat{\\beta}_{0}\\) is 36? Hint: modify the code above to do your analysis. # compute RSS for any pair of beta0 and beta1 in Galton&#39;s data set.seed(1983) galton_heights &lt;- GaltonFamilies %&gt;% filter(gender == &quot;male&quot;) %&gt;% group_by(family) %&gt;% sample_n(1) %&gt;% ungroup() %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) rss &lt;- function(beta0, beta1, data){ resid &lt;- galton_heights$son - (beta0+beta1*galton_heights$father) return(sum(resid^2)) } # plot RSS as a function of beta1 when beta0=36 beta1 = seq(0, 1, len=nrow(galton_heights)) results &lt;- data.frame(beta1 = beta1, rss = sapply(beta1, rss, beta0 = 36)) results %&gt;% ggplot(aes(beta1, rss)) + geom_line() + geom_line(aes(beta1, rss)) A. 0.65 B. 0.5 C. 0.2 D. 12 The least squares estimates for the parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_n\\) minimize the residual sum of squares. Load the Lahman library and filter the Teams data frame to the years 1961-2001. Run a linear model in R predicting the number of runs per game based on the number of bases on balls and the number of home runs. Remember to first limit your data to 1961-2001. What is the coefficient for bases on balls? fit &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(R_per_game = R / G, BB_per_game = BB / G, HR_per_game = HR / G) %&gt;% lm(R_per_game ~ BB_per_game + HR_per_game, data = .) summary(fit) ## ## Call: ## lm(formula = R_per_game ~ BB_per_game + HR_per_game, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.87325 -0.24507 -0.01449 0.23866 1.24218 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.74430 0.08236 21.18 &lt;2e-16 *** ## BB_per_game 0.38742 0.02701 14.34 &lt;2e-16 *** ## HR_per_game 1.56117 0.04896 31.89 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3484 on 1023 degrees of freedom ## Multiple R-squared: 0.6503, Adjusted R-squared: 0.6496 ## F-statistic: 951.2 on 2 and 1023 DF, p-value: &lt; 2.2e-16 A. 0.39 B. 1.56 C. 1.74 D. 0.027 The coefficient for bases on balls is 0.39; the coefficient for home runs is 1.56; the intercept is 1.74; the standard error for the BB coefficient is 0.027. We run a Monte Carlo simulation where we repeatedly take samples of N = 100 from the Galton heights data and compute the regression slope coefficients for each sample: B &lt;- 1000 N &lt;- 100 lse &lt;- replicate(B, { sample_n(galton_heights, N, replace = TRUE) %&gt;% lm(son ~ father, data = .) %&gt;% .$coef }) lse &lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) What does the central limit theorem tell us about the variables beta_0 and beta_1? A. They are approximately normally distributed. B. The expected value of each is the true value of \\(\\beta_0\\) and \\(\\beta_1\\) (assuming the Galton heights data is a complete population). C. The central limit theorem does not apply in this situation. D. It allows us to test the hypothesis that \\(\\beta_0 = 0\\) and \\(\\beta_0 = 1\\) In an earlier video, we ran the following linear model and looked at a summary of the results. $\\beta_0$ &gt; mod &lt;- lm(son ~ father, data = galton_heights) &gt; summary(mod) Call: lm(formula = son ~ father, data = galton_heights) Residuals: Min 1Q Median 3Q Max -5.902 -1.405 0.092 1.342 8.092 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 35.7125 4.5174 7.91 2.8e-13 *** father 0.5028 0.0653 7.70 9.5e-13 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 $\\beta_0$ What null hypothesis is the second p-value (the one in the father row) testing? A. \\(\\beta_1 = 1\\), where \\(\\beta_1\\) is the coefficient for the variable “father.” B. \\(\\beta_1 = 0.503\\), where \\(\\beta_1\\) is the coefficient for the variable “father.” C. \\(\\beta_1 = 0\\), where \\(\\beta_1\\) is the coefficient for the variable “father.” Which R code(s) below would properly plot the predictions and confidence intervals for our linear model of sons’ heights? Select ALL that apply. A. galton_heights %&gt;% ggplot(aes(father, son)) + geom_point() + geom_smooth() B. galton_heights %&gt;% ggplot(aes(father, son)) + geom_point() + geom_smooth(method = &quot;lm&quot;) C. model &lt;- lm(son ~ father, data = galton_heights) predictions &lt;- predict(model, interval = c(&quot;confidence&quot;), level = 0.95) data &lt;- as.tibble(predictions) %&gt;% bind_cols(father = galton_heights$father) ggplot(data, aes(x = father, y = fit)) + geom_line(color = &quot;blue&quot;, size = 1) + geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=0.2) + geom_point(data = galton_heights, aes(x = father, y = son)) D. model &lt;- lm(son ~ father, data = galton_heights) predictions &lt;- predict(model) data &lt;- as.tibble(predictions) %&gt;% bind_cols(father = galton_heights$father) ggplot(data, aes(x = father, y = fit)) + geom_line(color = &quot;blue&quot;, size = 1) + geom_point(data = galton_heights, aes(x = father, y = son)) 3.11 Assessment - Least Squares Estimates, part 2 In Questions 7 and 8, you’ll look again at female heights from GaltonFamilies. Define female_heights, a set of mother and daughter heights sampled from GaltonFamilies, as follows: set.seed(1989, sample.kind=&quot;Rounding&quot;) #if you are using R 3.6 or later ## Warning in set.seed(1989, sample.kind = &quot;Rounding&quot;): non-uniform &#39;Rounding&#39; sampler used options(digits = 3) # report 3 significant digits female_heights &lt;- GaltonFamilies %&gt;% filter(gender == &quot;female&quot;) %&gt;% group_by(family) %&gt;% sample_n(1) %&gt;% ungroup() %&gt;% select(mother, childHeight) %&gt;% rename(daughter = childHeight) Fit a linear regression model predicting the mothers’ heights using daughters’ heights. What is the slope of the model? fit &lt;- lm(mother ~ daughter, data = female_heights) fit$coef[2] ## daughter ## 0.31 What the intercept of the model? fit$coef[1] ## (Intercept) ## 44.2 Predict mothers’ heights using the model. What is the predicted height of the first mother in the dataset? predict(fit)[1] ## 1 ## 65.6 What is the actual height of the first mother in the dataset? female_heights$mother[1] ## [1] 67 We have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing how stable they are across the years. Because we have to pick players based on their previous performances, we will prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BBs. Before we get started, we want to generate two tables: one for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics, keeping only players with more than 100 plate appearances. Here is how we create the 2002 table: bat_02 &lt;- Batting %&gt;% filter(yearID == 2002) %&gt;% mutate(pa = AB + BB, singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) %&gt;% filter(pa &gt;= 100) %&gt;% select(playerID, singles, bb) Now compute a similar table but with rates computed over 1999-2001. Keep only rows from 1999-2001 where players have 100 or more plate appearances, calculate each player’s single rate and BB rate per season, then calculate the average single rate (mean_singles) and average BB rate (mean_bb) per player over those three seasons. How many players had a single rate mean_singles of greater than 0.2 per plate appearance over 1999-2001? bat_99_01 &lt;- Batting %&gt;% filter(yearID %in% 1999:2001) %&gt;% mutate(pa = AB + BB, singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) %&gt;% filter(pa &gt;= 100) %&gt;% group_by(playerID) %&gt;% summarize(mean_singles = mean(singles), mean_bb = mean(bb)) ## `summarise()` ungrouping output (override with `.groups` argument) sum(bat_99_01$mean_singles &gt; 0.2) ## [1] 46 How many players had a BB rate mean_bb of greater than 0.2 per plate appearance over 1999-2001? sum(bat_99_01$mean_bb &gt; 0.2) ## [1] 3 Use inner_join() to combine the `bat_02 table with the table of 1999-2001 rate averages you created in the previous question. What is the correlation between 2002 singles rates and 1999-2001 average singles rates? dat &lt;- inner_join(bat_02, bat_99_01) ## Joining, by = &quot;playerID&quot; cor(dat$singles, dat$mean_singles) ## [1] 0.551 What is the correlation between 2002 BB rates and 1999-2001 average BB rates? cor(dat$bb, dat$mean_bb) ## [1] 0.717 Make scatterplots of mean_singles versus singles and mean_bb versus bb. Are either of these distributions bivariate normal? dat %&gt;% ggplot(aes(singles, mean_singles)) + geom_point() dat %&gt;% ggplot(aes(bb, mean_bb)) + geom_point() A. Neither distribution is bivariate normal. B. singles and mean_singles are bivariate normal, but bb and mean_bb are not. C. bb and mean_bb are bivariate normal, but singles and mean_singles are not. D. Both distributions are bivariate normal. Fit a linear model to predict 2002 singles given 1999-2001 mean_singles. What is the coefficient of mean_singles, the slope of the fit? fit_singles &lt;- lm(singles ~ mean_singles, data = dat) fit_singles$coef[2] ## mean_singles ## 0.588 Fit a linear model to predict 2002 bb given 1999-2001 mean_bb. What is the coefficient of mean_bb, the slope of the fit? fit_bb &lt;- lm(bb ~ mean_bb, data = dat) fit_bb$coef[2] ## mean_bb ## 0.829 3.12 Advanced dplyr: Tibbles The textbook for this section is available here Key points Tibbles can be regarded as a modern version of data frames and are the default data structure in the tidyverse. Some functions that do not work properly with data frames do work with tibbles. Code # stratify by HR dat &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(HR = round(HR/G, 1), BB = BB/G, R = R/G) %&gt;% select(HR, BB, R) %&gt;% filter(HR &gt;= 0.4 &amp; HR&lt;=1.2) # calculate slope of regression lines to predict runs by BB in different HR strata dat %&gt;% group_by(HR) %&gt;% summarize(slope = cor(BB,R)*sd(R)/sd(BB)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 9 x 2 ## HR slope ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.4 0.734 ## 2 0.5 0.566 ## 3 0.6 0.412 ## 4 0.7 0.285 ## 5 0.8 0.365 ## 6 0.9 0.261 ## 7 1 0.512 ## 8 1.1 0.454 ## 9 1.2 0.440 # use lm to get estimated slopes - lm does not work with grouped tibbles dat %&gt;% group_by(HR) %&gt;% lm(R ~ BB, data = .) %&gt;% .$coef ## (Intercept) BB ## 2.198 0.638 # inspect a grouped tibble dat %&gt;% group_by(HR) %&gt;% head() ## # A tibble: 6 x 3 ## # Groups: HR [5] ## HR BB R ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.9 3.56 4.24 ## 2 0.7 3.97 4.47 ## 3 0.8 3.37 4.69 ## 4 1.1 3.46 4.42 ## 5 1 2.75 4.61 ## 6 0.9 3.06 4.58 dat %&gt;% group_by(HR) %&gt;% class() ## [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 3.13 Tibbles: Differences from Data Frames Key points Tibbles are more readable than data frames. If you subset a data frame, you may not get a data frame. If you subset a tibble, you always get a tibble. Tibbles can hold more complex objects such as lists or functions. Tibbles can be grouped. Code # inspect data frame and tibble head(Teams) ## yearID lgID teamID franchID divID Rank G Ghome W L DivWin WCWin LgWin WSWin R AB H X2B X3B HR BB SO SB CS HBP SF RA ER ERA CG SHO SV IPouts HA HRA BBA SOA E DP ## 1 1871 NA BS1 BNA &lt;NA&gt; 3 31 NA 20 10 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 401 1372 426 70 37 3 60 19 73 16 NA NA 303 109 3.55 22 1 3 828 367 2 42 23 243 24 ## 2 1871 NA CH1 CNA &lt;NA&gt; 2 28 NA 19 9 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 302 1196 323 52 21 10 60 22 69 21 NA NA 241 77 2.76 25 0 1 753 308 6 28 22 229 16 ## 3 1871 NA CL1 CFC &lt;NA&gt; 8 29 NA 10 19 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 249 1186 328 35 40 7 26 25 18 8 NA NA 341 116 4.11 23 0 0 762 346 13 53 34 234 15 ## 4 1871 NA FW1 KEK &lt;NA&gt; 7 19 NA 7 12 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 137 746 178 19 8 2 33 9 16 4 NA NA 243 97 5.17 19 1 0 507 261 5 21 17 163 8 ## 5 1871 NA NY2 NNA &lt;NA&gt; 5 33 NA 16 17 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 302 1404 403 43 21 1 33 15 46 15 NA NA 313 121 3.72 32 1 0 879 373 7 42 22 235 14 ## 6 1871 NA PH1 PNA &lt;NA&gt; 1 28 NA 21 7 &lt;NA&gt; &lt;NA&gt; Y &lt;NA&gt; 376 1281 410 66 27 9 46 23 56 12 NA NA 266 137 4.95 27 0 0 747 329 3 53 16 194 13 ## FP name park attendance BPF PPF teamIDBR teamIDlahman45 teamIDretro ## 1 0.834 Boston Red Stockings South End Grounds I NA 103 98 BOS BS1 BS1 ## 2 0.829 Chicago White Stockings Union Base-Ball Grounds NA 104 102 CHI CH1 CH1 ## 3 0.818 Cleveland Forest Citys National Association Grounds NA 96 100 CLE CL1 CL1 ## 4 0.803 Fort Wayne Kekiongas Hamilton Field NA 101 107 KEK FW1 FW1 ## 5 0.840 New York Mutuals Union Grounds (Brooklyn) NA 90 88 NYU NY2 NY2 ## 6 0.845 Philadelphia Athletics Jefferson Street Grounds NA 102 98 ATH PH1 PH1 as_tibble(Teams) ## # A tibble: 2,925 x 48 ## yearID lgID teamID franchID divID Rank G Ghome W L DivWin WCWin LgWin WSWin R AB H X2B X3B HR BB SO SB CS HBP SF RA ER ERA ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1871 NA BS1 BNA &lt;NA&gt; 3 31 NA 20 10 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 401 1372 426 70 37 3 60 19 73 16 NA NA 303 109 3.55 ## 2 1871 NA CH1 CNA &lt;NA&gt; 2 28 NA 19 9 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 302 1196 323 52 21 10 60 22 69 21 NA NA 241 77 2.76 ## 3 1871 NA CL1 CFC &lt;NA&gt; 8 29 NA 10 19 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 249 1186 328 35 40 7 26 25 18 8 NA NA 341 116 4.11 ## 4 1871 NA FW1 KEK &lt;NA&gt; 7 19 NA 7 12 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 137 746 178 19 8 2 33 9 16 4 NA NA 243 97 5.17 ## 5 1871 NA NY2 NNA &lt;NA&gt; 5 33 NA 16 17 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 302 1404 403 43 21 1 33 15 46 15 NA NA 313 121 3.72 ## 6 1871 NA PH1 PNA &lt;NA&gt; 1 28 NA 21 7 &lt;NA&gt; &lt;NA&gt; Y &lt;NA&gt; 376 1281 410 66 27 9 46 23 56 12 NA NA 266 137 4.95 ## 7 1871 NA RC1 ROK &lt;NA&gt; 9 25 NA 4 21 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 231 1036 274 44 25 3 38 30 53 10 NA NA 287 108 4.3 ## 8 1871 NA TRO TRO &lt;NA&gt; 6 29 NA 13 15 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 351 1248 384 51 34 6 49 19 62 24 NA NA 362 153 5.51 ## 9 1871 NA WS3 OLY &lt;NA&gt; 4 32 NA 15 15 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 310 1353 375 54 26 6 48 13 48 13 NA NA 303 137 4.37 ## 10 1872 NA BL1 BLC &lt;NA&gt; 2 58 NA 35 19 &lt;NA&gt; &lt;NA&gt; N &lt;NA&gt; 617 2571 753 106 31 14 29 28 53 18 NA NA 434 166 2.9 ## # … with 2,915 more rows, and 19 more variables: CG &lt;int&gt;, SHO &lt;int&gt;, SV &lt;int&gt;, IPouts &lt;int&gt;, HA &lt;int&gt;, HRA &lt;int&gt;, BBA &lt;int&gt;, SOA &lt;int&gt;, E &lt;int&gt;, DP &lt;int&gt;, FP &lt;dbl&gt;, name &lt;chr&gt;, ## # park &lt;chr&gt;, attendance &lt;int&gt;, BPF &lt;int&gt;, PPF &lt;int&gt;, teamIDBR &lt;chr&gt;, teamIDlahman45 &lt;chr&gt;, teamIDretro &lt;chr&gt; # Note that the function was formerly called as.tibble() # subsetting a data frame sometimes generates vectors class(Teams[,20]) ## [1] &quot;integer&quot; # subsetting a tibble always generates tibbles class(as_tibble(Teams[,20])) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # pulling a vector out of a tibble class(as_tibble(Teams)$HR) ## [1] &quot;integer&quot; # access a non-existing column in a data frame or a tibble Teams$hr ## NULL as_tibble(Teams)$HR ## [1] 3 10 7 2 1 9 3 6 6 14 0 1 7 0 2 4 4 5 0 0 9 0 6 13 0 5 4 8 2 1 1 17 4 2 7 6 2 2 15 0 2 0 2 7 ## [45] 7 5 0 0 0 0 9 8 4 2 6 2 7 2 4 0 6 4 9 1 2 3 5 3 2 8 2 20 3 4 8 12 5 4 3 20 4 7 7 8 5 8 12 5 ## [89] 12 7 17 11 5 7 18 4 15 15 20 5 19 9 5 11 18 11 12 16 8 5 34 13 8 15 34 13 14 24 6 20 3 21 13 7 2 39 32 17 16 36 19 142 ## [133] 10 16 40 36 26 31 20 6 17 0 22 22 26 14 7 21 2 7 11 32 0 8 2 6 4 23 17 14 22 54 26 25 19 16 21 30 20 6 5 17 8 8 16 24 ## [177] 53 45 53 19 20 21 18 21 26 16 20 30 23 31 25 53 80 14 37 55 33 27 48 21 29 47 20 39 47 19 25 56 77 12 32 51 34 19 14 55 31 16 14 36 ## [221] 30 20 47 42 79 25 36 52 62 18 22 52 43 44 42 58 25 20 2 13 43 34 31 58 67 31 27 21 16 27 15 25 66 24 23 49 20 35 31 48 14 24 30 23 ## [265] 52 53 60 40 22 20 28 17 13 46 55 21 29 57 19 30 30 34 26 44 26 18 39 50 38 45 37 27 45 65 32 29 32 19 61 80 37 10 23 33 42 103 65 61 ## [309] 37 42 45 40 48 54 59 25 41 54 55 36 29 34 32 61 27 39 55 23 28 36 34 20 28 37 40 49 27 37 45 19 24 45 38 22 16 40 31 40 25 32 36 12 ## [353] 17 53 18 19 18 32 34 33 14 13 36 17 27 39 27 13 12 40 23 31 29 47 47 26 48 33 33 23 29 26 36 24 37 32 28 32 18 38 12 29 26 19 35 24 ## [397] 29 39 33 33 42 19 14 14 6 18 33 22 6 38 5 18 29 10 47 48 15 25 14 9 28 31 12 20 18 32 12 34 12 8 17 26 15 24 14 22 21 27 11 31 ## [441] 27 31 23 15 10 24 10 29 29 17 11 12 27 18 13 39 23 24 16 22 16 20 22 13 25 16 7 20 16 12 10 14 17 32 12 12 20 9 26 18 18 22 5 13 ## [485] 15 11 11 23 15 22 12 19 10 18 12 14 28 17 3 19 14 18 19 20 13 21 11 25 20 17 8 20 16 14 4 20 22 10 19 26 16 21 12 25 10 15 9 43 ## [529] 25 31 7 34 23 9 28 31 20 19 22 33 12 15 9 35 28 37 20 54 21 20 30 41 25 35 60 49 17 26 16 29 32 35 17 43 21 12 19 47 18 22 43 39 ## [573] 19 27 20 17 39 32 24 59 27 16 24 30 8 33 73 35 18 15 19 32 18 42 31 35 38 19 52 42 16 10 25 33 39 30 12 29 62 18 34 17 26 33 18 36 ## [617] 14 36 14 17 40 25 50 53 15 20 23 28 17 24 31 16 58 24 20 19 23 20 12 14 28 22 17 46 14 16 17 42 35 19 42 20 14 25 12 14 25 22 18 17 ## [661] 26 13 25 39 27 17 38 9 15 26 4 15 10 13 8 21 15 9 13 13 20 22 25 15 5 27 4 33 25 24 25 21 20 24 23 40 45 35 42 17 31 18 24 22 ## [705] 28 23 37 34 18 35 30 46 115 44 64 16 50 32 36 17 59 61 35 37 20 42 58 75 134 82 88 37 67 83 42 45 56 32 45 42 45 32 54 80 95 111 116 52 ## [749] 98 107 45 34 62 32 42 90 45 59 41 85 105 53 112 49 82 63 26 30 72 25 41 66 36 41 35 95 98 63 94 44 67 67 22 41 64 41 38 86 44 52 50 114 ## [793] 110 76 100 78 110 109 56 32 40 16 32 66 35 27 36 73 121 61 75 44 72 90 43 28 39 37 36 74 29 26 51 109 158 56 57 54 55 84 29 38 66 52 24 92 ## [837] 32 34 62 118 133 89 85 52 63 113 40 28 99 33 37 139 34 62 110 136 142 122 153 60 46 100 48 47 122 66 63 171 74 72 82 143 152 125 126 86 75 104 57 37 ## [881] 71 34 27 84 21 71 43 101 155 118 81 41 76 60 49 53 110 63 36 69 47 78 80 116 160 172 122 48 67 76 61 50 62 54 43 72 34 50 57 82 144 139 60 39 ## [925] 64 57 60 51 79 83 71 101 55 100 74 126 135 144 56 52 62 104 51 69 59 75 74 88 73 93 106 123 104 112 92 66 73 86 32 86 33 67 60 76 82 123 94 97 ## [969] 182 72 103 60 79 88 62 100 37 63 67 96 73 103 150 111 174 94 103 47 71 94 47 98 61 54 67 65 110 113 137 125 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1925 entries ] # create a tibble with complex objects tibble(id = c(1, 2, 3), func = c(mean, median, sd)) ## # A tibble: 3 x 2 ## id func ## &lt;dbl&gt; &lt;list&gt; ## 1 1 &lt;fn&gt; ## 2 2 &lt;fn&gt; ## 3 3 &lt;fn&gt; 3.14 do The textbook for this section is available here Key points The do() function serves as a bridge between R functions, such as lm(), and the tidyverse. We have to specify a column when using the do() function, otherwise we will get an error. If the data frame being returned has more than one row, the rows will be concatenated appropriately. Code # use do to fit a regression line to each HR stratum dat %&gt;% group_by(HR) %&gt;% do(fit = lm(R ~ BB, data = .)) ## # A tibble: 9 x 2 ## # Rowwise: ## HR fit ## &lt;dbl&gt; &lt;list&gt; ## 1 0.4 &lt;lm&gt; ## 2 0.5 &lt;lm&gt; ## 3 0.6 &lt;lm&gt; ## 4 0.7 &lt;lm&gt; ## 5 0.8 &lt;lm&gt; ## 6 0.9 &lt;lm&gt; ## 7 1 &lt;lm&gt; ## 8 1.1 &lt;lm&gt; ## 9 1.2 &lt;lm&gt; # using do without a column name gives an error dat %&gt;% group_by(HR) %&gt;% do(lm(R ~ BB, data = .)) # define a function to extract slope from lm get_slope &lt;- function(data){ fit &lt;- lm(R ~ BB, data = data) data.frame(slope = fit$coefficients[2], se = summary(fit)$coefficient[2,2]) } # return the desired data frame dat %&gt;% group_by(HR) %&gt;% do(get_slope(.)) ## # A tibble: 9 x 3 ## # Groups: HR [9] ## HR slope se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.4 0.734 0.208 ## 2 0.5 0.566 0.110 ## 3 0.6 0.412 0.0974 ## 4 0.7 0.285 0.0705 ## 5 0.8 0.365 0.0653 ## 6 0.9 0.261 0.0751 ## 7 1 0.512 0.0751 ## 8 1.1 0.454 0.0855 ## 9 1.2 0.440 0.0801 # not the desired output: a column containing data frames dat %&gt;% group_by(HR) %&gt;% do(slope = get_slope(.)) ## # A tibble: 9 x 2 ## # Rowwise: ## HR slope ## &lt;dbl&gt; &lt;list&gt; ## 1 0.4 &lt;df[,2] [1 × 2]&gt; ## 2 0.5 &lt;df[,2] [1 × 2]&gt; ## 3 0.6 &lt;df[,2] [1 × 2]&gt; ## 4 0.7 &lt;df[,2] [1 × 2]&gt; ## 5 0.8 &lt;df[,2] [1 × 2]&gt; ## 6 0.9 &lt;df[,2] [1 × 2]&gt; ## 7 1 &lt;df[,2] [1 × 2]&gt; ## 8 1.1 &lt;df[,2] [1 × 2]&gt; ## 9 1.2 &lt;df[,2] [1 × 2]&gt; # data frames with multiple rows will be concatenated appropriately get_lse &lt;- function(data){ fit &lt;- lm(R ~ BB, data = data) data.frame(term = names(fit$coefficients), estimate = fit$coefficients, se = summary(fit)$coefficient[,2]) } dat %&gt;% group_by(HR) %&gt;% do(get_lse(.)) ## # A tibble: 18 x 4 ## # Groups: HR [9] ## HR term estimate se ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.4 (Intercept) 1.36 0.631 ## 2 0.4 BB 0.734 0.208 ## 3 0.5 (Intercept) 2.01 0.344 ## 4 0.5 BB 0.566 0.110 ## 5 0.6 (Intercept) 2.53 0.305 ## 6 0.6 BB 0.412 0.0974 ## 7 0.7 (Intercept) 3.21 0.225 ## 8 0.7 BB 0.285 0.0705 ## 9 0.8 (Intercept) 3.07 0.213 ## 10 0.8 BB 0.365 0.0653 ## 11 0.9 (Intercept) 3.54 0.251 ## 12 0.9 BB 0.261 0.0751 ## 13 1 (Intercept) 2.88 0.256 ## 14 1 BB 0.512 0.0751 ## 15 1.1 (Intercept) 3.21 0.300 ## 16 1.1 BB 0.454 0.0855 ## 17 1.2 (Intercept) 3.40 0.291 ## 18 1.2 BB 0.440 0.0801 3.15 broom The textbook for this section is available here Key points The broom package has three main functions, all of which extract information from the object returned by lm and return it in a tidyverse friendly data frame. The tidy() function returns estimates and related information as a data frame. The functions glance() and augment() relate to model specific and observation specific outcomes respectively. Code # use tidy to return lm estimates and related information as a data frame if(!require(broom)) install.packages(&quot;broom&quot;) library(broom) fit &lt;- lm(R ~ BB, data = dat) tidy(fit) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 2.20 0.113 19.4 1.12e-70 ## 2 BB 0.638 0.0344 18.5 1.35e-65 # add confidence intervals with tidy tidy(fit, conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 2.20 0.113 19.4 1.12e-70 1.98 2.42 ## 2 BB 0.638 0.0344 18.5 1.35e-65 0.570 0.705 # pipeline with lm, do, tidy dat %&gt;% group_by(HR) %&gt;% do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %&gt;% filter(term == &quot;BB&quot;) %&gt;% select(HR, estimate, conf.low, conf.high) ## # A tibble: 9 x 4 ## # Groups: HR [9] ## HR estimate conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.4 0.734 0.308 1.16 ## 2 0.5 0.566 0.346 0.786 ## 3 0.6 0.412 0.219 0.605 ## 4 0.7 0.285 0.146 0.425 ## 5 0.8 0.365 0.236 0.494 ## 6 0.9 0.261 0.112 0.409 ## 7 1 0.512 0.363 0.660 ## 8 1.1 0.454 0.284 0.624 ## 9 1.2 0.440 0.280 0.601 # make ggplots dat %&gt;% group_by(HR) %&gt;% do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %&gt;% filter(term == &quot;BB&quot;) %&gt;% select(HR, estimate, conf.low, conf.high) %&gt;% ggplot(aes(HR, y = estimate, ymin = conf.low, ymax = conf.high)) + geom_errorbar() + geom_point() # inspect with glance glance(fit) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.266 0.265 0.454 343. 1.35e-65 1 -596. 1199. 1214. 195. 947 949 3.16 Assessment - Tibbles, do, and broom, part 1 As seen in the videos, what problem do we encounter when we try to run a linear model on our baseball data, grouping by home runs? A. There is not enough data in some levels to run the model. B. The lm() function does not know how to handle grouped tibbles. C. The results of the lm() function cannot be put into a tidy format. Tibbles are similar to what other class in R? A. Vectors B. Matrices C. Data frames D. Lists What are some advantages of tibbles compared to data frames? A. Tibbles display better. B. If you subset a tibble, you always get back a tibble. C. Tibbles can have complex entries. D. Tibbles can be grouped. What are two advantages of the do command, when applied to the tidyverse? A. It is faster than normal functions. B. It returns useful error messages. C. It understands grouped tibbles. D. It always returns a data.frame. You want to take the tibble dat, which we used in the video on the do() function, and run the linear model R ~ BB for each strata of HR. Then you want to add three new columns to your grouped tibble: the coefficient, standard error, and p-value for the BB term in the model. You’ve already written the function get_slope(), shown below. get_slope &lt;- function(data) { fit &lt;- lm(R ~ BB, data = data) sum.fit &lt;- summary(fit) data.frame(slope = sum.fit$coefficients[2, &quot;Estimate&quot;], se = sum.fit$coefficients[2, &quot;Std. Error&quot;], pvalue = sum.fit$coefficients[2, &quot;Pr(&gt;|t|)&quot;]) } What additional code could you write to accomplish your goal? dat %&gt;% group_by(HR) %&gt;% do(get_slope(.)) ## # A tibble: 9 x 4 ## # Groups: HR [9] ## HR slope se pvalue ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.4 0.734 0.208 1.54e- 3 ## 2 0.5 0.566 0.110 3.02e- 6 ## 3 0.6 0.412 0.0974 4.80e- 5 ## 4 0.7 0.285 0.0705 7.93e- 5 ## 5 0.8 0.365 0.0653 9.13e- 8 ## 6 0.9 0.261 0.0751 6.85e- 4 ## 7 1 0.512 0.0751 3.28e-10 ## 8 1.1 0.454 0.0855 1.03e- 6 ## 9 1.2 0.440 0.0801 1.07e- 6 A. dat %&gt;% group_by(HR) %&gt;% do(get_slope) B. dat %&gt;% group_by(HR) %&gt;% do(get_slope(.)) C. dat %&gt;% group_by(HR) %&gt;% do(slope = get_slope(.)) D. dat %&gt;% do(get_slope(.)) The output of a broom function is always what? A. A data.frame B. A list C. A vector You want to know whether the relationship between home runs and runs per game varies by baseball league. You create the following dataset: dat &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(HR = HR/G, R = R/G) %&gt;% select(lgID, HR, BB, R) What code would help you quickly answer this question? dat %&gt;% group_by(lgID) %&gt;% do(tidy(lm(R ~ HR, data = .), conf.int = T)) %&gt;% filter(term == &quot;HR&quot;) ## # A tibble: 2 x 8 ## # Groups: lgID [2] ## lgID term estimate std.error statistic p.value conf.low conf.high ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AL HR 1.90 0.0734 25.9 1.29e-95 1.75 2.04 ## 2 NL HR 1.76 0.0671 26.2 1.16e-95 1.62 1.89 A. dat %&gt;% group_by(lgID) %&gt;% do(tidy(lm(R ~ HR, data = .), conf.int = T)) %&gt;% filter(term == &quot;HR&quot;) B. dat %&gt;% group_by(lgID) %&gt;% do(glance(lm(R ~ HR, data = .))) C. dat %&gt;% do(tidy(lm(R ~ HR, data = .), conf.int = T)) %&gt;% filter(term == &quot;HR&quot;) D. dat %&gt;% group_by(lgID) %&gt;% do(mod = lm(R ~ HR, data = .)) 3.17 Assessment - Tibbles, do, and broom, part 2 We have investigated the relationship between fathers’ heights and sons’ heights. But what about other parent-child relationships? Does one parent’s height have a stronger association with child height? How does the child’s gender affect this relationship in heights? Are any differences that we observe statistically significant? The galton dataset is a sample of one male and one female child from each family in the GaltonFamilies dataset. The pair column denotes whether the pair is father and daughter, father and son, mother and daughter, or mother and son. Create the galton dataset using the code below: data(&quot;GaltonFamilies&quot;) set.seed(1) # if you are using R 3.5 or earlier set.seed(1, sample.kind = &quot;Rounding&quot;) # if you are using R 3.6 or later ## Warning in set.seed(1, sample.kind = &quot;Rounding&quot;): non-uniform &#39;Rounding&#39; sampler used galton &lt;- GaltonFamilies %&gt;% group_by(family, gender) %&gt;% sample_n(1) %&gt;% ungroup() %&gt;% gather(parent, parentHeight, father:mother) %&gt;% mutate(child = ifelse(gender == &quot;female&quot;, &quot;daughter&quot;, &quot;son&quot;)) %&gt;% unite(pair, c(&quot;parent&quot;, &quot;child&quot;)) galton ## # A tibble: 710 x 8 ## family midparentHeight children childNum gender childHeight pair parentHeight ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 001 75.4 4 2 female 69.2 father_daughter 78.5 ## 2 001 75.4 4 1 male 73.2 father_son 78.5 ## 3 002 73.7 4 4 female 65.5 father_daughter 75.5 ## 4 002 73.7 4 2 male 72.5 father_son 75.5 ## 5 003 72.1 2 2 female 68 father_daughter 75 ## 6 003 72.1 2 1 male 71 father_son 75 ## 7 004 72.1 5 5 female 63 father_daughter 75 ## 8 004 72.1 5 2 male 68.5 father_son 75 ## 9 005 69.1 6 5 female 62.5 father_daughter 75 ## 10 005 69.1 6 1 male 72 father_son 75 ## # … with 700 more rows Group by pair and summarize the number of observations in each group. How many father-daughter pairs are in the dataset? How many mother-son pairs are in the dataset? galton %&gt;% group_by(pair) %&gt;% summarize(n = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 2 ## pair n ## &lt;chr&gt; &lt;int&gt; ## 1 father_daughter 176 ## 2 father_son 179 ## 3 mother_daughter 176 ## 4 mother_son 179 Calculate the correlation coefficients for fathers and daughters, fathers and sons, mothers and daughters and mothers and sons. Which pair has the strongest correlation in heights? galton %&gt;% group_by(pair) %&gt;% summarize(cor = cor(parentHeight, childHeight)) %&gt;% filter(cor == max(cor)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 1 x 2 ## pair cor ## &lt;chr&gt; &lt;dbl&gt; ## 1 father_son 0.430 Which pair has the weakest correlation in heights? galton %&gt;% group_by(pair) %&gt;% summarize(cor = cor(parentHeight, childHeight)) %&gt;% filter(cor == min(cor)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 1 x 2 ## pair cor ## &lt;chr&gt; &lt;dbl&gt; ## 1 mother_son 0.343 Question 10 has two parts. The information here applies to both parts. Use lm() and the broom package to fit regression lines for each parent-child pair type. Compute the least squares estimates, standard errors, confidence intervals and p-values for the parentHeight coefficient for each pair. 10a. What is the estimate of the father-daughter coefficient? galton %&gt;% group_by(pair) %&gt;% do(tidy(lm(childHeight ~ parentHeight, data = .), conf.int = TRUE)) %&gt;% filter(term == &quot;parentHeight&quot;, pair == &quot;father_daughter&quot;) %&gt;% pull(estimate) ## [1] 0.345 For every 1-inch increase in mother’s height, how many inches does the typical son’s height increase? galton %&gt;% group_by(pair) %&gt;% do(tidy(lm(childHeight ~ parentHeight, data = .), conf.int = TRUE)) %&gt;% filter(term == &quot;parentHeight&quot;, pair == &quot;mother_son&quot;) %&gt;% pull(estimate) ## [1] 0.381 10b. Which sets of parent-child heights are significantly correlated at a p-value cut off of .05? galton %&gt;% group_by(pair) %&gt;% do(tidy(lm(childHeight ~ parentHeight, data = .), conf.int = TRUE)) %&gt;% filter(term == &quot;parentHeight&quot; &amp; p.value &lt; .05) ## # A tibble: 4 x 8 ## # Groups: pair [4] ## pair term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 father_daughter parentHeight 0.345 0.0599 5.77 0.0000000356 0.227 0.464 ## 2 father_son parentHeight 0.443 0.0700 6.33 0.00000000194 0.305 0.581 ## 3 mother_daughter parentHeight 0.394 0.0720 5.47 0.000000156 0.252 0.536 ## 4 mother_son parentHeight 0.381 0.0784 4.86 0.00000259 0.226 0.535 A. father-daughter B. father-son C. mother-daughter D. mother-son Which of the following statements are true? A. All of the confidence intervals overlap each other. B. At least one confidence interval covers zero. C. The confidence intervals involving mothers’ heights are larger than the confidence intervals involving fathers’ heights. D. The confidence intervals involving daughters’ heights are larger than the confidence intervals involving sons’ heights. E. The data are consistent with inheritance of height being independent of the child’s gender. F. The data are consistent with inheritance of height being independent of the parent’s gender. 3.18 Building a Better Offensive Metric for Baseball The textbook for this section is available here Code # linear regression with two variables fit &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(BB = BB/G, HR = HR/G, R = R/G) %&gt;% lm(R ~ BB + HR, data = .) tidy(fit, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.74 0.0824 21.2 7.62e- 83 1.58 1.91 ## 2 BB 0.387 0.0270 14.3 1.20e- 42 0.334 0.440 ## 3 HR 1.56 0.0490 31.9 1.78e-155 1.47 1.66 # regression with BB, singles, doubles, triples, HR fit &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(BB = BB / G, singles = (H - X2B - X3B - HR) / G, doubles = X2B / G, triples = X3B / G, HR = HR / G, R = R / G) %&gt;% lm(R ~ BB + singles + doubles + triples + HR, data = .) coefs &lt;- tidy(fit, conf.int = TRUE) coefs ## # A tibble: 6 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.77 0.0862 -32.1 4.76e-157 -2.94 -2.60 ## 2 BB 0.371 0.0117 31.6 1.87e-153 0.348 0.394 ## 3 singles 0.519 0.0127 40.8 8.67e-217 0.494 0.544 ## 4 doubles 0.771 0.0226 34.1 8.44e-171 0.727 0.816 ## 5 triples 1.24 0.0768 16.1 2.12e- 52 1.09 1.39 ## 6 HR 1.44 0.0243 59.3 0. 1.40 1.49 # predict number of runs for each team in 2002 and plot Teams %&gt;% filter(yearID %in% 2002) %&gt;% mutate(BB = BB/G, singles = (H-X2B-X3B-HR)/G, doubles = X2B/G, triples =X3B/G, HR=HR/G, R=R/G) %&gt;% mutate(R_hat = predict(fit, newdata = .)) %&gt;% ggplot(aes(R_hat, R, label = teamID)) + geom_point() + geom_text(nudge_x=0.1, cex = 2) + geom_abline() # average number of team plate appearances per game pa_per_game &lt;- Batting %&gt;% filter(yearID == 2002) %&gt;% group_by(teamID) %&gt;% summarize(pa_per_game = sum(AB+BB)/max(G)) %&gt;% pull(pa_per_game) %&gt;% mean ## `summarise()` ungrouping output (override with `.groups` argument) # compute per-plate-appearance rates for players available in 2002 using previous data players &lt;- Batting %&gt;% filter(yearID %in% 1999:2001) %&gt;% group_by(playerID) %&gt;% mutate(PA = BB + AB) %&gt;% summarize(G = sum(PA)/pa_per_game, BB = sum(BB)/G, singles = sum(H-X2B-X3B-HR)/G, doubles = sum(X2B)/G, triples = sum(X3B)/G, HR = sum(HR)/G, AVG = sum(H)/sum(AB), PA = sum(PA)) %&gt;% filter(PA &gt;= 300) %&gt;% select(-G) %&gt;% mutate(R_hat = predict(fit, newdata = .)) ## `summarise()` ungrouping output (override with `.groups` argument) # plot player-specific predicted runs qplot(R_hat, data = players, geom = &quot;histogram&quot;, binwidth = 0.5, color = I(&quot;black&quot;)) # add 2002 salary of each player players &lt;- Salaries %&gt;% filter(yearID == 2002) %&gt;% select(playerID, salary) %&gt;% right_join(players, by=&quot;playerID&quot;) # add defensive position position_names &lt;- c(&quot;G_p&quot;,&quot;G_c&quot;,&quot;G_1b&quot;,&quot;G_2b&quot;,&quot;G_3b&quot;,&quot;G_ss&quot;,&quot;G_lf&quot;,&quot;G_cf&quot;,&quot;G_rf&quot;) tmp_tab &lt;- Appearances %&gt;% filter(yearID == 2002) %&gt;% group_by(playerID) %&gt;% summarize_at(position_names, sum) %&gt;% ungroup() pos &lt;- tmp_tab %&gt;% select(position_names) %&gt;% apply(., 1, which.max) ## Note: Using an external vector in selections is ambiguous. ## ℹ Use `all_of(position_names)` instead of `position_names` to silence this message. ## ℹ See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;. ## This message is displayed once per session. players &lt;- data_frame(playerID = tmp_tab$playerID, POS = position_names[pos]) %&gt;% mutate(POS = str_to_upper(str_remove(POS, &quot;G_&quot;))) %&gt;% filter(POS != &quot;P&quot;) %&gt;% right_join(players, by=&quot;playerID&quot;) %&gt;% filter(!is.na(POS) &amp; !is.na(salary)) ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. # add players&#39; first and last names players &lt;- Master %&gt;% select(playerID, nameFirst, nameLast, debut) %&gt;% mutate(debut = as.Date(debut)) %&gt;% right_join(players, by=&quot;playerID&quot;) # top 10 players players %&gt;% select(nameFirst, nameLast, POS, salary, R_hat) %&gt;% arrange(desc(R_hat)) %&gt;% top_n(10) ## Selecting by R_hat ## nameFirst nameLast POS salary R_hat ## 1 Barry Bonds LF 15000000 9.05 ## 2 Todd Helton 1B 5000000 8.23 ## 3 Manny Ramirez LF 15462727 8.20 ## 4 Sammy Sosa RF 15000000 8.19 ## 5 Larry Walker RF 12666667 8.15 ## 6 Jason Giambi 1B 10428571 7.99 ## 7 Chipper Jones LF 11333333 7.64 ## 8 Brian Giles LF 8063003 7.57 ## 9 Albert Pujols LF 600000 7.54 ## 10 Nomar Garciaparra SS 9000000 7.51 # players with a higher metric have higher salaries players %&gt;% ggplot(aes(salary, R_hat, color = POS)) + geom_point() + scale_x_log10() # remake plot without players that debuted after 1998 if(!require(lubridate)) install.packages(&quot;lubridate&quot;) ## Loading required package: lubridate ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(lubridate) players %&gt;% filter(year(debut) &lt; 1998) %&gt;% ggplot(aes(salary, R_hat, color = POS)) + geom_point() + scale_x_log10() 3.19 Building a Better Offensive Metric for Baseball: Linear Programming A way to actually pick the players for the team can be done using what computer scientists call linear programming. Although we don’t go into this topic in detail in this course, we include the code anyway: if(!require(reshape2)) install.packages(&quot;reshape2&quot;) ## Loading required package: reshape2 ## ## Attaching package: &#39;reshape2&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths if(!require(lpSolve)) install.packages(&quot;lpSolve&quot;) ## Loading required package: lpSolve library(reshape2) library(lpSolve) players &lt;- players %&gt;% filter(debut &lt;= &quot;1997-01-01&quot; &amp; debut &gt; &quot;1988-01-01&quot;) constraint_matrix &lt;- acast(players, POS ~ playerID, fun.aggregate = length) ## Using R_hat as value column: use value.var to override. npos &lt;- nrow(constraint_matrix) constraint_matrix &lt;- rbind(constraint_matrix, salary = players$salary) constraint_dir &lt;- c(rep(&quot;==&quot;, npos), &quot;&lt;=&quot;) constraint_limit &lt;- c(rep(1, npos), 50*10^6) lp_solution &lt;- lp(&quot;max&quot;, players$R_hat, constraint_matrix, constraint_dir, constraint_limit, all.int = TRUE) This algorithm chooses these 9 players: our_team &lt;- players %&gt;% filter(lp_solution$solution == 1) %&gt;% arrange(desc(R_hat)) our_team %&gt;% select(nameFirst, nameLast, POS, salary, R_hat) ## nameFirst nameLast POS salary R_hat ## 1 Larry Walker RF 12666667 8.15 ## 2 Nomar Garciaparra SS 9000000 7.51 ## 3 Luis Gonzalez LF 4333333 7.40 ## 4 Mike Piazza C 10571429 7.16 ## 5 Jim Edmonds CF 7333333 6.90 ## 6 Phil Nevin 3B 2600000 6.75 ## 7 Greg Colbrunn 1B 1800000 6.44 ## 8 Terry Shumpert 2B 775000 6.04 We note that these players all have above average BB and HR rates while the same is not true for singles. my_scale &lt;- function(x) (x - median(x))/mad(x) players %&gt;% mutate(BB = my_scale(BB), singles = my_scale(singles), doubles = my_scale(doubles), triples = my_scale(triples), HR = my_scale(HR), AVG = my_scale(AVG), R_hat = my_scale(R_hat)) %&gt;% filter(playerID %in% our_team$playerID) %&gt;% select(nameFirst, nameLast, BB, singles, doubles, triples, HR, AVG, R_hat) %&gt;% arrange(desc(R_hat)) ## nameFirst nameLast BB singles doubles triples HR AVG R_hat ## 1 Larry Walker 1.0605 0.6554 0.922 1.562 1.566 2.835 2.904 ## 2 Nomar Garciaparra 0.0274 1.6371 3.118 0.336 0.625 3.197 2.244 ## 3 Luis Gonzalez 0.7046 0.0000 1.413 0.537 1.355 1.829 2.133 ## 4 Mike Piazza 0.3129 -0.0547 -0.242 -1.274 2.035 1.252 1.891 ## 5 Jim Edmonds 1.8074 -1.1409 0.674 -0.674 1.264 0.579 1.621 ## 6 Phil Nevin 0.4909 -0.6479 0.764 -1.098 1.548 0.728 1.463 ## 7 Greg Colbrunn 0.2703 0.6546 0.784 0.585 0.475 1.375 1.148 ## 8 Terry Shumpert -0.1576 0.1221 1.326 3.908 -0.123 0.859 0.744 3.20 On Base Plus Slugging (OPS) Key point The on-base-percentage plus slugging percentage (OPS) metric is: \\(\\frac{\\mbox{BB}}{\\mbox{PA}} + \\frac{(\\mbox{Singles} + 2\\mbox{Doubles} + 3\\mbox{Triples} + 4\\mbox{HR})}{\\mbox{AB}}\\) 3.21 Regression Fallacy The textbook for this section is available here Key points Regression can bring about errors in reasoning, especially when interpreting individual observations. The example showed in the video demonstrates that the “sophomore slump” observed in the data is caused by regressing to the mean. Code The code to create a table with player ID, their names, and their most played position: playerInfo &lt;- Fielding %&gt;% group_by(playerID) %&gt;% arrange(desc(G)) %&gt;% slice(1) %&gt;% ungroup %&gt;% left_join(Master, by=&quot;playerID&quot;) %&gt;% select(playerID, nameFirst, nameLast, POS) The code to create a table with only the ROY award winners and add their batting statistics: ROY &lt;- AwardsPlayers %&gt;% filter(awardID == &quot;Rookie of the Year&quot;) %&gt;% left_join(playerInfo, by=&quot;playerID&quot;) %&gt;% rename(rookie_year = yearID) %&gt;% right_join(Batting, by=&quot;playerID&quot;) %&gt;% mutate(AVG = H/AB) %&gt;% filter(POS != &quot;P&quot;) The code to keep only the rookie and sophomore seasons and remove players who did not play sophomore seasons: ROY &lt;- ROY %&gt;% filter(yearID == rookie_year | yearID == rookie_year+1) %&gt;% group_by(playerID) %&gt;% mutate(rookie = ifelse(yearID == min(yearID), &quot;rookie&quot;, &quot;sophomore&quot;)) %&gt;% filter(n() == 2) %&gt;% ungroup %&gt;% select(playerID, rookie_year, rookie, nameFirst, nameLast, AVG) The code to use the spread function to have one column for the rookie and sophomore years batting averages: ROY &lt;- ROY %&gt;% spread(rookie, AVG) %&gt;% arrange(desc(rookie)) ROY ## # A tibble: 102 x 6 ## playerID rookie_year nameFirst nameLast rookie sophomore ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mccovwi01 1959 Willie McCovey 0.354 0.238 ## 2 suzukic01 2001 Ichiro Suzuki 0.350 0.321 ## 3 bumbral01 1973 Al Bumbry 0.337 0.233 ## 4 lynnfr01 1975 Fred Lynn 0.331 0.314 ## 5 pujolal01 2001 Albert Pujols 0.329 0.314 ## 6 troutmi01 2012 Mike Trout 0.326 0.323 ## 7 braunry02 2007 Ryan Braun 0.324 0.285 ## 8 olivato01 1964 Tony Oliva 0.323 0.321 ## 9 hargrmi01 1974 Mike Hargrove 0.323 0.303 ## 10 darkal01 1948 Al Dark 0.322 0.276 ## # … with 92 more rows The code to calculate the proportion of players who have a lower batting average their sophomore year: mean(ROY$sophomore - ROY$rookie &lt;= 0) ## [1] 0.686 The code to do the similar analysis on all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year): two_years &lt;- Batting %&gt;% filter(yearID %in% 2013:2014) %&gt;% group_by(playerID, yearID) %&gt;% filter(sum(AB) &gt;= 130) %&gt;% summarize(AVG = sum(H)/sum(AB)) %&gt;% ungroup %&gt;% spread(yearID, AVG) %&gt;% filter(!is.na(`2013`) &amp; !is.na(`2014`)) %&gt;% left_join(playerInfo, by=&quot;playerID&quot;) %&gt;% filter(POS!=&quot;P&quot;) %&gt;% select(-POS) %&gt;% arrange(desc(`2013`)) %&gt;% select(nameFirst, nameLast, `2013`, `2014`) ## `summarise()` regrouping output by &#39;playerID&#39; (override with `.groups` argument) two_years ## # A tibble: 312 x 4 ## nameFirst nameLast `2013` `2014` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Miguel Cabrera 0.348 0.313 ## 2 Hanley Ramirez 0.345 0.283 ## 3 Michael Cuddyer 0.331 0.332 ## 4 Scooter Gennett 0.324 0.289 ## 5 Joe Mauer 0.324 0.277 ## 6 Mike Trout 0.323 0.287 ## 7 Chris Johnson 0.321 0.263 ## 8 Freddie Freeman 0.319 0.288 ## 9 Yasiel Puig 0.319 0.296 ## 10 Yadier Molina 0.319 0.282 ## # … with 302 more rows The code to see what happens to the worst performers of 2013: arrange(two_years, `2013`) ## # A tibble: 312 x 4 ## nameFirst nameLast `2013` `2014` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Danny Espinosa 0.158 0.219 ## 2 Dan Uggla 0.179 0.149 ## 3 Jeff Mathis 0.181 0.2 ## 4 B. J. Upton 0.184 0.208 ## 5 Adam Rosales 0.190 0.262 ## 6 Aaron Hicks 0.192 0.215 ## 7 Chris Colabello 0.194 0.229 ## 8 J. P. Arencibia 0.194 0.177 ## 9 Tyler Flowers 0.195 0.241 ## 10 Ryan Hanigan 0.198 0.218 ## # … with 302 more rows The code to see the correlation for performance in two separate years: qplot(`2013`, `2014`, data = two_years) summarize(two_years, cor(`2013`,`2014`)) ## # A tibble: 1 x 1 ## `cor(\\`2013\\`, \\`2014\\`)` ## &lt;dbl&gt; ## 1 0.460 3.22 Measurement Error Models The textbook for this section is available here Key points Up to now, all our linear regression examples have been applied to two or more random variables. We assume the pairs are bivariate normal and use this to motivate a linear model. Another use for linear regression is with measurement error models, where it is common to have a non-random covariate (such as time). Randomness is introduced from measurement error rather than sampling or natural variability. Code The code to use dslabs function rfalling_object to generate simulations of dropping balls: falling_object &lt;- rfalling_object() The code to draw the trajectory of the ball: falling_object %&gt;% ggplot(aes(time, observed_distance)) + geom_point() + ylab(&quot;Distance in meters&quot;) + xlab(&quot;Time in seconds&quot;) The code to use the lm() function to estimate the coefficients: fit &lt;- falling_object %&gt;% mutate(time_sq = time^2) %&gt;% lm(observed_distance~time+time_sq, data=.) tidy(fit) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 55.0 0.434 127. 9.15e-19 ## 2 time 0.888 0.620 1.43 1.80e- 1 ## 3 time_sq -5.08 0.184 -27.7 1.61e-11 The code to check if the estimated parabola fits the data: augment(fit) %&gt;% ggplot() + geom_point(aes(time, observed_distance)) + geom_line(aes(time, .fitted), col = &quot;blue&quot;) The code to see the summary statistic of the regression: tidy(fit, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 55.0 0.434 127. 9.15e-19 54.1 56.0 ## 2 time 0.888 0.620 1.43 1.80e- 1 -0.476 2.25 ## 3 time_sq -5.08 0.184 -27.7 1.61e-11 -5.49 -4.68 3.23 Assessment - Regression and Baseball, part 1 What is the final linear model (in the video “Building a Better Offensive Metric for Baseball”) we used to predict runs scored per game? A. lm(R ~ BB + HR) B. lm(HR ~ BB + singles + doubles + triples) C. lm(R ~ BB + singles + doubles + triples + HR) D. lm(R ~ singles + doubles + triples + HR) We want to estimate runs per game scored by individual players, not just by teams. What summary metric do we calculate to help estimate this? Look at the code from the video “Building a Metter Offensive Metric for Baseball” for a hint: pa_per_game &lt;- Batting %&gt;% filter(yearID == 2002) %&gt;% group_by(teamID) %&gt;% summarize(pa_per_game = sum(AB+BB)/max(G)) %&gt;% .$pa_per_game %&gt;% mean A. pa_per_game: the mean number of plate appearances per team per game for each team B. pa_per_game: the mean number of plate appearances per game for each player C. pa_per_game: the number of plate appearances per team per game, averaged across all teams Imagine you have two teams. Team A is comprised of batters who, on average, get two bases on balls, four singles, one double, and one home run. Team B is comprised of batters who, on average, get one base on balls, six singles, two doubles, and one triple. Which team scores more runs, as predicted by our model? A. Team A B. Team B C. Tie D. Impossible to know The on-base-percentage plus slugging percentage (OPS) metric gives the most weight to: A. Singles B. Doubles C. Triples D. Home Runs What statistical concept properly explains the “sophomore slump”? A. Regression to the mean B. Law of averages C. Normal distribution In our model of time vs. observed_distance in the video “Measurement Error Models”, the randomness of our data was due to: A. sampling B. natural variability C. measurement error Which of the following are important assumptions about the measurement errors in this experiment? A. The measurement error is random B. The measurement error is independent C. The measurement error has the same distribution for each time i Which of the following scenarios would violate an assumption of our measurement error model? A. The experiment was conducted on the moon. B. There was one position where it was particularly difficult to see the dropped ball. C. The experiment was only repeated 10 times, not 100 times. 3.24 Assessment - Regression and Baseball, part 2 Question 9 has two parts. Use the information below to answer both parts. Use the Teams data frame from the Lahman package. Fit a multivariate linear regression model to obtain the effects of BB and HR on Runs (R) in 1971. Use the tidy() function in the broom package to obtain the results in a data frame. 9a. What is the estimate for the effect of BB on runs? Teams %&gt;% filter(yearID == 1971) %&gt;% lm(R ~ BB + HR, data = .) %&gt;% tidy() %&gt;% filter(term == &quot;BB&quot;) %&gt;% pull(estimate) ## [1] 0.414 What is the estimate for the effect of HR on runs? Teams %&gt;% filter(yearID == 1971) %&gt;% lm(R ~ BB + HR, data = .) %&gt;% tidy() %&gt;% filter(term == &quot;HR&quot;) %&gt;% pull(estimate) ## [1] 1.3 9b. Interpret the p-values for the estimates using a cutoff of 0.05. Which of the following is the correct interpretation? fit &lt;- Teams %&gt;% filter(yearID %in% 1971) %&gt;% mutate(BB = BB/G, HR = HR/G, R = R/G) %&gt;% lm(R ~ BB + HR, data = .) tidy(fit, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.60 0.665 2.40 0.0256 0.215 2.98 ## 2 BB 0.412 0.203 2.03 0.0552 -0.0101 0.835 ## 3 HR 1.29 0.427 3.03 0.00639 0.406 2.18 A. Both BB and HR have a nonzero effect on runs. B. HR has a significant effect on runs, but the evidence is not strong enough to suggest BB also does. C. BB has a significant effect on runs, but the evidence is not strong enough to suggest HR also does. D. Neither BB nor HR have a statistically significant effect on runs. Repeat the above exercise to find the effects of BB and HR on runs (R) for every year from 1961 to 2018 using do() and the broom package. Make a scatterplot of the estimate for the effect of BB on runs over time and add a trend line with confidence intervals. res &lt;- Teams %&gt;% filter(yearID %in% 1961:2018) %&gt;% group_by(yearID) %&gt;% do(tidy(lm(R ~ BB + HR, data = .))) %&gt;% ungroup() res %&gt;% filter(term == &quot;BB&quot;) %&gt;% ggplot(aes(yearID, estimate)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Fill in the blank to complete the statement: The effect of BB on runs has increased over time. Fit a linear model on the results from Question 10 to determine the effect of year on the impact of BB. For each additional year, by what value does the impact of BB on runs change? res %&gt;% filter(term == &quot;BB&quot;) %&gt;% lm(estimate ~ yearID, data = .) %&gt;% tidy() %&gt;% filter(term == &quot;yearID&quot;) %&gt;% pull(estimate) ## [1] 0.00355 What is the p-value for this effect? res %&gt;% filter(term == &quot;BB&quot;) %&gt;% lm(estimate ~ yearID, data = .) %&gt;% tidy() %&gt;% filter(term == &quot;yearID&quot;) %&gt;% pull(p.value) ## [1] 0.00807 3.25 Assessment - Linear Models This assessment has 6 multi-part questions that will all use the setup below. Game attendance in baseball varies partly as a function of how well a team is playing. Load the Lahman library. The Teams data frame contains an attendance column. This is the total attendance for the season. To calculate average attendance, divide by the number of games played, as follows: Teams_small &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;% mutate(avg_attendance = attendance/G) Use linear models to answer the following 3-part question about Teams_small. 1a. Use runs (R) per game to predict average attendance. For every 1 run scored per game, average attendance increases by how much? # find regression line predicting attendance from R and take slope Teams_small %&gt;% mutate(R_per_game = R/G) %&gt;% lm(avg_attendance ~ R_per_game, data = .) %&gt;% .$coef %&gt;% .[2] ## R_per_game ## 4117 Use home runs (HR) per game to predict average attendance. For every 1 home run hit per game, average attendance increases by how much? Teams_small %&gt;% mutate(HR_per_game = HR/G) %&gt;% lm(avg_attendance ~ HR_per_game, data = .) %&gt;% .$coef %&gt;% .[2] ## HR_per_game ## 8113 1b. Use number of wins to predict average attendance; do not normalize for number of games. For every game won in a season, how much does average attendance increase? Teams_small %&gt;% lm(avg_attendance ~ W, data = .) %&gt;% .$coef %&gt;% .[2] ## W ## 121 Suppose a team won zero games in a season. Predict the average attendance. Teams_small %&gt;% lm(avg_attendance ~ W, data = .) %&gt;% .$coef %&gt;% .[1] ## (Intercept) ## 1129 1c. Use year to predict average attendance. How much does average attendance increase each year? Teams_small %&gt;% lm(avg_attendance ~ yearID, data = .) %&gt;% .$coef %&gt;% .[2] ## yearID ## 244 Game wins, runs per game and home runs per game are positively correlated with attendance. We saw in the course material that runs per game and home runs per game are correlated with each other. Are wins and runs per game or wins and home runs per game correlated? What is the correlation coefficient for wins and runs per game? cor(Teams_small$W, Teams_small$R/Teams_small$G) ## [1] 0.412 What is the correlation coefficient for wins and home runs per game? cor(Teams_small$W, Teams_small$HR/Teams_small$G) ## [1] 0.274 Stratify Teams_small by wins: divide number of wins by 10 and then round to the nearest integer. Keep only strata 5 through 10, which have 20 or more data points. Use the stratified dataset to answer this three-part question. 3a. How many observations are in the 8 win strata? (Note that due to division and rounding, these teams have 75-85 wins.) dat &lt;- Teams_small %&gt;% mutate(W_strata = round(W/10)) %&gt;% filter(W_strata &gt;= 5 &amp; W_strata &lt;= 10) sum(dat$W_strata == 8) ## [1] 338 3b. Calculate the slope of the regression line predicting average attendance given runs per game for each of the win strata. Which win stratum has the largest regression line slope? # calculate slope of regression line after stratifying by R per game dat %&gt;% group_by(W_strata) %&gt;% summarize(slope = cor(R/G, avg_attendance)*sd(avg_attendance)/sd(R/G)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 6 x 2 ## W_strata slope ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5 4362. ## 2 6 4343. ## 3 7 3888. ## 4 8 3128. ## 5 9 3701. ## 6 10 3107. A. 5 B. 6 C. 7 D. 8 E. 9 F. 10 Calculate the slope of the regression line predicting average attendance given HR per game for each of the win strata. Which win stratum has the largest regression line slope? # calculate slope of regression line after stratifying by HR per game dat %&gt;% group_by(W_strata) %&gt;% summarize(slope = cor(HR/G, avg_attendance)*sd(avg_attendance)/sd(HR/G)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 6 x 2 ## W_strata slope ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5 10192. ## 2 6 7032. ## 3 7 8931. ## 4 8 6301. ## 5 9 5863. ## 6 10 4917. A. 5 B. 6 C. 7 D. 8 E. 9 F. 10 3c. Which of the followng are true about the effect of win strata on average attendance? A. Across all win strata, runs per game are positively correlated with average attendance. B. Runs per game have the strongest effect on attendance when a team wins many games. C. After controlling for number of wins, home runs per game are not correlated with attendance. D. Home runs per game have the strongest effect on attendance when a team does not win many games. E. Among teams with similar numbers of wins, teams with more home runs per game have larger average attendance. Fit a multivariate regression determining the effects of runs per game, home runs per game, wins, and year on average attendance. Use the original Teams_small wins column, not the win strata from question 3. What is the estimate of the effect of runs per game on average attendance? fit &lt;- Teams_small %&gt;% mutate(R_per_game = R/G, HR_per_game = HR/G) %&gt;% lm(avg_attendance ~ R_per_game + HR_per_game + W + yearID, data = .) tidy(fit) %&gt;% filter(term == &quot;R_per_game&quot;) %&gt;% pull(estimate) ## [1] 322 What is the estimate of the effect of home runs per game on average attendance? tidy(fit) %&gt;% filter(term == &quot;HR_per_game&quot;) %&gt;% pull(estimate) ## [1] 1798 What is the estimate of the effect of number of wins in a season on average attendance? tidy(fit) %&gt;% filter(term == &quot;W&quot;) %&gt;% pull(estimate) ## [1] 117 Use the multivariate regression model from Question 4. Suppose a team averaged 5 runs per game, 1.2 home runs per game, and won 80 games in a season. What would this team’s average attendance be in 2002? predict(fit, data.frame(R_per_game = 5, HR_per_game = 1.2, W = 80, yearID = 2002)) ## 1 ## 16149 What would this team’s average attendance be in 1960? predict(fit, data.frame(R_per_game = 5, HR_per_game = 1.2, W = 80, yearID = 1960)) ## 1 ## 6505 Use your model from Question 4 to predict average attendance for teams in 2002 in the original Teams data frame. What is the correlation between the predicted attendance and actual attendance? newdata &lt;- Teams %&gt;% filter(yearID == 2002) %&gt;% mutate(avg_attendance = attendance/G, R_per_game = R/G, HR_per_game = HR/G) preds &lt;- predict(fit, newdata) cor(preds, newdata$avg_attendance) ## [1] 0.519 "],
["section-3-confounding-overview.html", "4 Section 3 - Confounding Overview 4.1 Correlation is Not Causation: Spurious Correlation 4.2 Correlation is Not Causation: Outliers 4.3 Correlation is Not Causation: Reversing Cause and Effect 4.4 Correlation is Not Causation: Confounders 4.5 Simpson’s Paradox 4.6 Assessment - Correlation is Not Causation 4.7 Assessment - Confounding", " 4 Section 3 - Confounding Overview In the Confounding section, you will learn what is perhaps the most important lesson of statistics: that correlation is not causation. After completing this section, you will be able to: Identify examples of spurious correlation and explain how data dredging can lead to spurious correlation. Explain how outliers can drive correlation and learn to adjust for outliers using Spearman correlation. Explain how reversing cause and effect can lead to associations being confused with causation. Understand how confounders can lead to the misinterpretation of associations. Explain and give examples of Simpson’s Paradox. This section has one part: Correlation is Not Causation. 4.1 Correlation is Not Causation: Spurious Correlation The textbook for this section is available here Key points Association/correlation is not causation. p-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. Code # generate the Monte Carlo simulation N &lt;- 25 g &lt;- 1000000 sim_data &lt;- tibble(group = rep(1:g, each = N), x = rnorm(N * g), y = rnorm(N * g)) # calculate correlation between X,Y for each group res &lt;- sim_data %&gt;% group_by(group) %&gt;% summarize(r = cor(x, y)) %&gt;% arrange(desc(r)) ## `summarise()` ungrouping output (override with `.groups` argument) res ## # A tibble: 1,000,000 x 2 ## group r ## &lt;int&gt; &lt;dbl&gt; ## 1 840003 0.794 ## 2 767028 0.791 ## 3 971856 0.776 ## 4 212248 0.768 ## 5 60200 0.761 ## 6 27045 0.756 ## 7 114409 0.756 ## 8 755537 0.754 ## 9 422986 0.753 ## 10 789165 0.747 ## # … with 999,990 more rows # plot points from the group with maximum correlation sim_data %&gt;% filter(group == res$group[which.max(res$r)]) %&gt;% ggplot(aes(x, y)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; # histogram of correlation in Monte Carlo simulations res %&gt;% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = &quot;black&quot;) # linear regression on group with maximum correlation library(broom) sim_data %&gt;% filter(group == res$group[which.max(res$r)]) %&gt;% do(tidy(lm(y ~ x, data = .))) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.121 0.116 -1.04 0.309 ## 2 x 0.812 0.130 6.27 0.00000213 4.2 Correlation is Not Causation: Outliers The textbook for this section is available here Key points Correlations can be caused by outliers. The Spearman correlation is calculated based on the ranks of data. Code # simulate independent X, Y and standardize all except entry 23 set.seed(1985) x &lt;- rnorm(100,100,1) y &lt;- rnorm(100,84,1) x[-23] &lt;- scale(x[-23]) y[-23] &lt;- scale(y[-23]) # plot shows the outlier qplot(x, y, alpha = 0.5) # outlier makes it appear there is correlation cor(x,y) ## [1] 0.988 cor(x[-23], y[-23]) ## [1] -0.0442 # use rank instead qplot(rank(x), rank(y)) cor(rank(x), rank(y)) ## [1] 0.00251 # Spearman correlation with cor function cor(x, y, method = &quot;spearman&quot;) ## [1] 0.00251 4.3 Correlation is Not Causation: Reversing Cause and Effect The textbook for this section is available here Key points Another way association can be confused with causation is when the cause and effect are reversed. In the Galton data, when father and son were reversed in the regression, the model was technically correct. The estimates and p-values were obtained correctly as well. What was incorrect was the interpretation of the model. Code # cause and effect reversal using son heights to predict father heights data(&quot;GaltonFamilies&quot;) GaltonFamilies %&gt;% filter(childNum == 1 &amp; gender == &quot;male&quot;) %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) %&gt;% do(tidy(lm(father ~ son, data = .))) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 34.0 4.57 7.44 4.31e-12 ## 2 son 0.499 0.0648 7.70 9.47e-13 4.4 Correlation is Not Causation: Confounders The textbook for this section is available here Key points If X and Y are correlated, we call Z a confounder if changes in Z causes changes in both X and Y. Code # UC-Berkeley admission data data(admissions) admissions ## major gender admitted applicants ## 1 A men 62 825 ## 2 B men 63 560 ## 3 C men 37 325 ## 4 D men 33 417 ## 5 E men 28 191 ## 6 F men 6 373 ## 7 A women 82 108 ## 8 B women 68 25 ## 9 C women 34 593 ## 10 D women 35 375 ## 11 E women 24 393 ## 12 F women 7 341 # percent men and women accepted admissions %&gt;% group_by(gender) %&gt;% summarize(percentage = round(sum(admitted*applicants)/sum(applicants),1)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## gender percentage ## &lt;chr&gt; &lt;dbl&gt; ## 1 men 44.5 ## 2 women 30.3 # test whether gender and admission are independent admissions %&gt;% group_by(gender) %&gt;% summarize(total_admitted = round(sum(admitted / 100 * applicants)), not_admitted = sum(applicants) - sum(total_admitted)) %&gt;% select(-gender) %&gt;% do(tidy(chisq.test(.))) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 1 x 4 ## statistic p.value parameter method ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 91.6 1.06e-21 1 Pearson&#39;s Chi-squared test with Yates&#39; continuity correction # percent admissions by major admissions %&gt;% select(major, gender, admitted) %&gt;% spread(gender, admitted) %&gt;% mutate(women_minus_men = women - men) ## major men women women_minus_men ## 1 A 62 82 20 ## 2 B 63 68 5 ## 3 C 37 34 -3 ## 4 D 33 35 2 ## 5 E 28 24 -4 ## 6 F 6 7 1 # plot total percent admitted to major versus percent women applicants admissions %&gt;% group_by(major) %&gt;% summarize(major_selectivity = sum(admitted * applicants) / sum(applicants), percent_women_applicants = sum(applicants * (gender==&quot;women&quot;)) / sum(applicants) * 100) %&gt;% ggplot(aes(major_selectivity, percent_women_applicants, label = major)) + geom_text() ## `summarise()` ungrouping output (override with `.groups` argument) # plot number of applicants admitted and not admissions %&gt;% mutate(yes = round(admitted/100*applicants), no = applicants - yes) %&gt;% select(-applicants, -admitted) %&gt;% gather(admission, number_of_students, -c(&quot;major&quot;, &quot;gender&quot;)) %&gt;% ggplot(aes(gender, number_of_students, fill = admission)) + geom_bar(stat = &quot;identity&quot;, position = &quot;stack&quot;) + facet_wrap(. ~ major) admissions %&gt;% mutate(percent_admitted = admitted * applicants/sum(applicants)) %&gt;% ggplot(aes(gender, y = percent_admitted, fill = major)) + geom_bar(stat = &quot;identity&quot;, position = &quot;stack&quot;) # condition on major and then look at differences admissions %&gt;% ggplot(aes(major, admitted, col = gender, size = applicants)) + geom_point() # average difference by major admissions %&gt;% group_by(gender) %&gt;% summarize(average = mean(admitted)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## gender average ## &lt;chr&gt; &lt;dbl&gt; ## 1 men 38.2 ## 2 women 41.7 4.5 Simpson’s Paradox The textbook for this section is available here Key point Simpson’s Paradox happens when we see the sign of the correlation flip when comparing the entire dataset with specific strata. 4.6 Assessment - Correlation is Not Causation In the video, we ran one million tests of correlation for two random variables, X and Y. How many of these correlations would you expect to have a significant p-value (p&gt;0.05), just by chance? A. 5,000 B. 50,000 C. 100,000 D. It’s impossible to know The chance of finding a correlation when none exists is \\(0.05*1.000.000\\) chances Which of the following are examples of p-hacking? A. Looking for associations between an outcome and several exposures and only reporting the one that is significant. B. Trying several different models and selecting the one that yields the smallest p-value. C. Repeating an experiment multiple times and only reporting the one with the smallest p-value. D. Using a Monte Carlo simulations in an analysis. The Spearman correlation coefficient is robust to outliers because: A. It drops outliers before calculating correlation. B. It is the correlation of standardized values. C. It calculates correlation between ranks, not values. What can you do to determine if you are misinterpreting results because of a confounder? A. Nothing, if the p-value says the result is significant, then it is. B. More closely examine the results by stratifying and plotting the data. C. Always assume that you are misinterpreting the results. D. Use linear models to tease out a confounder. Look again at the admissions data using ?admissions. What important characteristic of the table variables do you need to know to understand the calculations used in this video? Select the best answer. A. The data is from 1973. B. The columns “major” and “gender” are of class character, while “admitted” and “applicants” are numeric. C. The data is from the “dslabs” package. D. The column “admitted” is the percent of student admitted, while the column “applicants” is the total number of applicants. In the example in the video, major selectivity confounds the relationship between UC Berkley admission rates and gender because: A. It was harder for women to be admitted to UC Berkeley. B. Major selectivity is associated with both admission rates and with gender, as women tended to apply to more selective majors. C. Some majors are more selective than others D. Major selectivity is not a confounder. Admission rates at UC Berkeley are an example of Simpson’s Paradox because: A. It appears that men have higher a higher admission rate than women, however, after we stratify by major, we see that on average women have a higher admission rate than men. B. It was a paradox that women were being admitted at a lower rate than men. C. The relationship between admissions and gender is confounded by major selectivity. 4.7 Assessment - Confounding For this set of exercises, we examine the data from a 2014 PNAS paper that analyzed success rates from funding agencies in the Netherlands and concluded: “our results reveal gender bias favoring male applicants over female applicants in the prioritization of their”quality of researcher\" (but not “quality of proposal”) evaluations and success rates, as well as in the language used in instructional and evaluation materials.\" A response was published a few months later titled No evidence that gender contributes to personal research funding success in The Netherlands: A reaction to Van der Lee and Ellemers, which concluded: However, the overall gender effect borders on statistical significance, despite the large sample. Moreover, their conclusion could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines (i.e., with low application success rates for both men and women), then an analysis across all disciplines could incorrectly show “evidence” of gender inequality. Who is right here: the original paper or the response? Here, you will examine the data and come to your own conclusion. The main evidence for the conclusion of the original paper comes down to a comparison of the percentages. The information we need was originally in Table S1 in the paper, which we include in dslabs: data(&quot;research_funding_rates&quot;) research_funding_rates ## discipline applications_total applications_men applications_women awards_total awards_men awards_women success_rates_total success_rates_men success_rates_women ## 1 Chemical sciences 122 83 39 32 22 10 26.2 26.5 25.6 ## 2 Physical sciences 174 135 39 35 26 9 20.1 19.3 23.1 ## 3 Physics 76 67 9 20 18 2 26.3 26.9 22.2 ## 4 Humanities 396 230 166 65 33 32 16.4 14.3 19.3 ## 5 Technical sciences 251 189 62 43 30 13 17.1 15.9 21.0 ## 6 Interdisciplinary 183 105 78 29 12 17 15.8 11.4 21.8 ## 7 Earth/life sciences 282 156 126 56 38 18 19.9 24.4 14.3 ## 8 Social sciences 834 425 409 112 65 47 13.4 15.3 11.5 ## 9 Medical sciences 505 245 260 75 46 29 14.9 18.8 11.2 Construct a two-by-two table of gender (men/women) by award status (awarded/not) using the total numbers across all disciplines. What is the number of men not awarded? What is the number of women not awarded? two_by_two &lt;- research_funding_rates %&gt;% select(-discipline) %&gt;% summarize_all(funs(sum)) %&gt;% summarize(yes_men = awards_men, no_men = applications_men - awards_men, yes_women = awards_women, no_women = applications_women - awards_women) %&gt;% gather %&gt;% separate(key, c(&quot;awarded&quot;, &quot;gender&quot;)) %&gt;% spread(gender, value) two_by_two ## awarded men women ## 1 no 1345 1011 ## 2 yes 290 177 Use the two-by-two table from Question 1 to compute the percentages of men awarded versus women awarded. What is the percentage of men awarded? two_by_two %&gt;% mutate(men = round(men/sum(men)*100, 1), women = round(women/sum(women)*100, 1)) %&gt;% filter(awarded == &quot;yes&quot;) %&gt;% pull(men) ## [1] 17.7 What is the percentage of women awarded? two_by_two %&gt;% mutate(men = round(men/sum(men)*100, 1), women = round(women/sum(women)*100, 1)) %&gt;% filter(awarded == &quot;yes&quot;) %&gt;% pull(women) ## [1] 14.9 Run a chi-squared test on the two-by-two table to determine whether the difference in the two success rates is significant. (You can use tidy() to turn the output of chisq.test() into a data frame as well.) What is the p-value of the difference in funding rate? two_by_two %&gt;% select(-awarded) %&gt;% chisq.test() %&gt;% tidy() %&gt;% pull(p.value) ## [1] 0.0509 There may be an association between gender and funding. But can we infer causation here? Is gender bias causing this observed difference? The response to the original paper claims that what we see here is similar to the UC Berkeley admissions example. Specifically they state that this “could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines, then an analysis across all disciplines could incorrectly show ‘evidence’ of gender inequality.” To settle this dispute, use this dataset with number of applications, awards, and success rate for each gender: dat &lt;- research_funding_rates %&gt;% mutate(discipline = reorder(discipline, success_rates_total)) %&gt;% rename(success_total = success_rates_total, success_men = success_rates_men, success_women = success_rates_women) %&gt;% gather(key, value, -discipline) %&gt;% separate(key, c(&quot;type&quot;, &quot;gender&quot;)) %&gt;% spread(type, value) %&gt;% filter(gender != &quot;total&quot;) dat ## discipline gender applications awards success ## 1 Social sciences men 425 65 15.3 ## 2 Social sciences women 409 47 11.5 ## 3 Medical sciences men 245 46 18.8 ## 4 Medical sciences women 260 29 11.2 ## 5 Interdisciplinary men 105 12 11.4 ## 6 Interdisciplinary women 78 17 21.8 ## 7 Humanities men 230 33 14.3 ## 8 Humanities women 166 32 19.3 ## 9 Technical sciences men 189 30 15.9 ## 10 Technical sciences women 62 13 21.0 ## 11 Earth/life sciences men 156 38 24.4 ## 12 Earth/life sciences women 126 18 14.3 ## 13 Physical sciences men 135 26 19.3 ## 14 Physical sciences women 39 9 23.1 ## 15 Chemical sciences men 83 22 26.5 ## 16 Chemical sciences women 39 10 25.6 ## 17 Physics men 67 18 26.9 ## 18 Physics women 9 2 22.2 To check if this is a case of Simpson’s paradox, plot the success rates versus disciplines, which have been ordered by overall success, with colors to denote the genders and size to denote the number of applications. dat %&gt;% ggplot(aes(discipline, success, size = applications, color = gender)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + geom_point() In which fields do men have a higher success rate than women? A. Chemical sciences B. Earth/life sciences C. Humanities D. Interdisciplinary E. Medical sciences F. Physical sciences G. Physics H. Social sciences I. Technical sciences Which two fields have the most applications from women? A. Chemical sciences B. Earth/life sciences C. Humanities D. Interdisciplinary E. Medical sciences F. Physical sciences G. Physics H. Social sciences I. Technical sciences Which two fields have the lowest overall funding rates? A. Chemical sciences B. Earth/life sciences C. Humanities D. Interdisciplinary E. Medical sciences F. Physical sciences G. Physics H. Social sciences I. Technical sciences "]
]
